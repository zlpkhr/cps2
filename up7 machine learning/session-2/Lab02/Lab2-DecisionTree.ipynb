{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 02 : Implementation of Decision Tree Algorithms From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full Name** : Azamat Zulpykhar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task :\n",
    "\n",
    "- Students must complete the code to create two classifiers: ID3 and CART.\n",
    "\n",
    "### Tools & libraries : \n",
    "- Python, Jupyter, pandas, scikit-learn, numpy, timeit, graphviz\n",
    "\n",
    "### DATASETS : \n",
    "- PlayOut (With nominal features)\n",
    "- PlayOut_num (With both nominal and numerical features) \n",
    "- [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INTRODUCTION\n",
    "\n",
    "A decision tree is a very simple machine learning model. It was widely used in the 1960s-1980s for building expert systems. It is widely known and used in many companies to facilitate decision making and risk analysis.\n",
    "\n",
    "Given dataset with many features, the decision starts by selecting one of these features (the most appropriate and relevant one, following some defined criteria) ; if this is not sufficient, another feature is selected, and so on.\n",
    "\n",
    "At their creation, the rules were introduced manually, for this reason this model lost its popularity after the 1980s, but the appearance of mathematical methods for building decision trees brings this model back into the battle of machine learning (ML) algorithms. There are several ML algorithms for building decision trees. So far in this lab, we will learn about :\n",
    "\n",
    "- **ID3 (Iterative Dichotomiser 3)** [[JR Quinlan, 1986](https://link.springer.com/content/pdf/10.1007/BF00116251.pdf)]: It was developed in 1986 by Ross Quinlan. It can be applied only on nominal features. It is used for ranking.\n",
    "- **C4.5** [[JR Quinlan, 1993](https://books.google.fr/books?hl=fr&lr=&id=b3ujBQAAQBAJ&oi=fnd&pg=PP1&dq=C4.+5:+Programs+for+machine+learning&ots=sR7pRTJuC2&sig=hmtbed3-eUljdUvf6eoM2vfrbHQ&redir_esc=y#v=onepage&q=C4.%205%3A%20Programs%20for%20machine%20learning&f=false)] : an extension of ID3 by Ross Quinlan. It can be applied to all types of features. It is used for classification.\n",
    "- **C5.0** [[JR Quinlan, 2004](https://www.rulequest.com/see5-info.html)] : a commercial extension of C4.5, again by Ross Quinlan.\n",
    "- **CART (Classification and Regression Trees)** [Breiman et al. , 1984](Refrence) : like C4.5 but uses other metrics. Also, the algorithm supports regression.\n",
    "\n",
    "### 1.1 The general algorithm for creating a decision tree:\n",
    "\n",
    "1. Determine the best feature in the training data set.\n",
    "2. Divide the training data into subsets containing the possible values of the best feature.\n",
    "3. Recursively generate new decision trees using the subsets of data created.\n",
    "4. When the data can no longer be classified, stop.\n",
    "\n",
    "### 1.2 Benefits\n",
    "- They are simple to understand and interpret. \n",
    "- The decision trees can be visualised. Also, the results obtained can be easily explained.\n",
    "- They can work on data with lightwheith preprocessing of data e.g., they do not need data normalisation.\n",
    "- They accept both numerical and nominal data. Other learning algorithms are specialised in one type of data.\n",
    "- They perform well even if their assumptions are somewhat violated by the actual model from which the data was generated.\n",
    "\n",
    "### 1.3 Limitations\n",
    "- They can be as complex, they do not generalise well (they generally tend to over-fit the data); this can be fixed by setting the minimum number of samples in the leaves and the maximum depth of the tree.\n",
    "- They can be unstable because of variations in the data.\n",
    "- There are some concepts that are a bit difficult to learn from decision trees. They are not easy to express, for example: XOR operation.\n",
    "- They can be biased to the dominant class. So you have to balance the data before training the system.\n",
    "- It is not guaranteed that the optimal decision tree will be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Impelementation\n",
    "\n",
    "In this lab, we will implement ID3 for the nominal features and CART (ranking) for the numerical features only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version : 2.1.1\n",
      "pandas version : 2.2.3\n",
      "matplotlib version : 3.9.2\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy             as np\n",
    "import pandas            as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "# Check our libraries versions\n",
    "print(\"numpy version :\",np.__version__) \n",
    "print(\"pandas version :\",pd.__version__)\n",
    "print(\"matplotlib version :\",matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support for type hints\n",
    "from typing          import Tuple, List, Type, Union\n",
    "from collections.abc import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section we will improve our understanding of the machine learning algorithms seen in class by implementing them from scratch. \n",
    "- To do this, we will use the numpy library which is useful for computations, especially matrix operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ID3 (Iterative Dichotomiser 3) [[JR Quinlan, 1986](https://link.springer.com/content/pdf/10.1007/BF00116251.pdf)]\n",
    "\n",
    "- This algorithm only works on nominal features.\n",
    "- So, if we have continuous features, we have to apply the discretization.\n",
    "- Also, it is used for classification task only (not for regression task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Reading and preparation of data\n",
    "- We will use a small dataset named \"PlayOut\" that contains only nominal features (variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weather</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind</th>\n",
       "      <th>playOut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sunny</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunny</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cloudy</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rainy</td>\n",
       "      <td>cool</td>\n",
       "      <td>high</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rainy</td>\n",
       "      <td>mild</td>\n",
       "      <td>dry</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rainy</td>\n",
       "      <td>mild</td>\n",
       "      <td>dry</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cloudy</td>\n",
       "      <td>mild</td>\n",
       "      <td>dry</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sunny</td>\n",
       "      <td>cool</td>\n",
       "      <td>high</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sunny</td>\n",
       "      <td>mild</td>\n",
       "      <td>dry</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rainy</td>\n",
       "      <td>cool</td>\n",
       "      <td>dry</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sunny</td>\n",
       "      <td>cool</td>\n",
       "      <td>dry</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cloudy</td>\n",
       "      <td>cool</td>\n",
       "      <td>high</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cloudy</td>\n",
       "      <td>hot</td>\n",
       "      <td>dry</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rainy</td>\n",
       "      <td>cool</td>\n",
       "      <td>high</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   weather temperature humidity wind playOut\n",
       "0    sunny         hot     high   no      no\n",
       "1    sunny         hot     high  yes      no\n",
       "2   cloudy         hot     high   no     yes\n",
       "3    rainy        cool     high   no     yes\n",
       "4    rainy        mild      dry   no     yes\n",
       "5    rainy        mild      dry  yes      no\n",
       "6   cloudy        mild      dry  yes     yes\n",
       "7    sunny        cool     high   no      no\n",
       "8    sunny        mild      dry   no     yes\n",
       "9    rainy        cool      dry   no     yes\n",
       "10   sunny        cool      dry  yes     yes\n",
       "11  cloudy        cool     high  yes     yes\n",
       "12  cloudy         hot      dry   no     yes\n",
       "13   rainy        cool     high  yes      no"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataset \"PlayOut.csv\" using pandas.read_csv\n",
    "df_playOut = pd.read_csv(\"data/PlayOut.csv\")\n",
    "# Print the DataFrame df_playOut\n",
    "df_playOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to separate data into : (1) inputs noted X, and (2) outputs(labels, classes) noted Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_playOut : \n",
      " [['sunny' 'hot' 'high' 'no']\n",
      " ['sunny' 'hot' 'high' 'yes']\n",
      " ['cloudy' 'hot' 'high' 'no']\n",
      " ['rainy' 'cool' 'high' 'no']\n",
      " ['rainy' 'mild' 'dry' 'no']\n",
      " ['rainy' 'mild' 'dry' 'yes']\n",
      " ['cloudy' 'mild' 'dry' 'yes']\n",
      " ['sunny' 'cool' 'high' 'no']\n",
      " ['sunny' 'mild' 'dry' 'no']\n",
      " ['rainy' 'cool' 'dry' 'no']\n",
      " ['sunny' 'cool' 'dry' 'yes']\n",
      " ['cloudy' 'cool' 'high' 'yes']\n",
      " ['cloudy' 'hot' 'dry' 'no']\n",
      " ['rainy' 'cool' 'high' 'yes']]\n",
      "\n",
      "Y_playOut : \n",
      " ['no' 'no' 'yes' 'yes' 'yes' 'no' 'yes' 'no' 'yes' 'yes' 'yes' 'yes' 'yes'\n",
      " 'no']\n"
     ]
    }
   ],
   "source": [
    "# Get the first columns that represents the features\n",
    "X_playOut = df_playOut.iloc[:, :-1].values\n",
    "\n",
    "# Get the last column of labels (classes) \n",
    "Y_playOut = df_playOut.iloc[:,-1].values   \n",
    "\n",
    "# Print X\n",
    "print(\"X_playOut : \\n\",X_playOut[:, :])\n",
    "\n",
    "# Print Y\n",
    "print(\"\\nY_playOut : \\n\",Y_playOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Probability function\n",
    "\n",
    "Let :\n",
    "\n",
    "$S$ : Be a set of values (we represent that with a strings vector in python).\n",
    "\n",
    "$v$ : Be a value (in our case it represents a string value)\n",
    "\n",
    "**The probability of occurrence of a value $v$ in $S$ is** : the number of occurrences of $v$ in $S$ divided by the total number of elements of the set $S$, as written in the following formula : \n",
    "\n",
    "$$p(v/S) = \\frac{|\\{x \\in S / x = v\\}|}{|S|}$$\n",
    "\n",
    "For example, let's take the column \"PlayOut\". \n",
    "The number of \"yes\" is 9 and the total number of rows is 14, therefore :\n",
    "$$p(PlayOut=yes) = \\frac{9}{14} = 0.6428571428571429$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# TODO \n",
    "# calculate the probability of occurrence of a value val in a set S\n",
    "# Note : if division always returns 0, try applying float(x) to the numerator or denominator\n",
    "\n",
    "def P(S: np.ndarray, val: str) -> float:\n",
    "    '''\n",
    "    Input : \n",
    "        S :  is a string vector (array)\n",
    "        v : String \n",
    "    Output : \n",
    "        results :float, the probability of occurrence.\n",
    "    '''\n",
    "    ### BEGIN : Write your code here\n",
    "    \n",
    "    xc = 0\n",
    "\n",
    "    for x in S:\n",
    "        if x == val:\n",
    "            xc+=1\n",
    "\n",
    "    return xc/(len(S) +  sys.float_info.epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we do a hand calculation of the following probabilities with a precision of \n",
    "5 digits after the decimal point (no rounding), here's what we get :\n",
    "- $P(PlayOut = yes) = 9/14 = 0.64285$\n",
    "- $P(weather = sunny) = 5/14 = 0.35714$\n",
    "- $P(weather = cloudy) = 4/14 = 0.28571$\n",
    "- $P(weather = rainy) = 5/14 = 0.35714$\n",
    "\n",
    "In the next cell, we are going to check if our function returns correct answers for this probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(playOut = yes)    =  0.6428571428571429\n",
      "P(weather = sunny)  =  0.35714285714285715\n",
      "P(weather = cloudy) =  0.2857142857142857\n",
      "P(weather = rainy)  =  0.35714285714285715\n"
     ]
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TESTING (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Results : \n",
    "# P(playOut = yes)    =  0.6428571428571429\n",
    "# P(weather = sunny)  =  0.35714285714285715\n",
    "# P(weather = cloudy) =  0.2857142857142857\n",
    "# P(weather = rainy)  =  0.35714285714285715\n",
    "#---------------------------------------------------------------------\n",
    "print(\"P(playOut = yes)    = \",P(Y_playOut, \"yes\"))\n",
    "print(\"P(weather = sunny)  = \",P(X_playOut[:,0], \"sunny\")) \n",
    "print(\"P(weather = cloudy) = \",P(X_playOut[:, 0], \"cloudy\"))\n",
    "print(\"P(weather = rainy)  = \",P(X_playOut[:,0], \"rainy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Uncertainty of a set\n",
    "\n",
    "- **The concept of information entropy** was introduced by Claude Shannon in his 1948 paper \"A Mathematical Theory of Communication\"[(Shannon, Claude E., 1948)](https://pure.mpg.de/rest/items/item_2383164/component/file_2383163/content), and is also referred to as Shannon entropy.\n",
    "\n",
    "- **Shannon entropy** is the amount of information contained in a source of information; the more different information the source carries, the greater the entropy (or uncertainty about what the source carries).\n",
    "\n",
    "- Thus, a set with an entropy of 0 contains the same values.\n",
    "\n",
    "So, Let : \n",
    "- $S$ be a set of values \n",
    "- $V$ a set of unique values of $S$ \n",
    "\n",
    "The entropy of $S$ is calculated as follows: \n",
    "$$H(S) = - \\sum\\limits_{v \\in V} p(v/S) \\log_2 p(v/S)$$\n",
    "\n",
    "For example, the column \"PlayOut\" contains two values \"yes\" and \"no\".  Its entopy is :\n",
    "$$H(PlayOut) = - \\frac{9}{14} * \\log_2(\\frac{9}{14}) - \\frac{5}{14} * \\log_2(\\frac{5}{14}) = 0.9402859586706309$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the entropy function\n",
    "# Hint : The numpy function ''numpy.log2'' computes Base-2 logarithm (log2) of a value, vector or matrix \n",
    "# See also : https://numpy.org/doc/stable/reference/generated/numpy.log2.html\n",
    "\n",
    "def H(S: np.ndarray) -> float:\n",
    "    '''\n",
    "    Input : \n",
    "        S : ndarray, set of values\n",
    "    Output : \n",
    "        entropy : float, the information entropy score of the set S\n",
    "    '''\n",
    "    vals = np.unique(S) # Get the set of unique values of the array S\n",
    "    entropy  = 0        # Set \n",
    "    \n",
    "    ### BEGIN : Write your code here\n",
    "    # Hint : Apply the formula using a loop\n",
    "\n",
    "    for val in vals:\n",
    "        entropy += (0-P(S,val)) * np.log2(P(S,val))\n",
    "\n",
    "    ### END\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hand calculation :** \n",
    "\n",
    "\n",
    "$H(weather) = -P(temps = sunny)*log2(P(weather = sunny)) - P(weather = cloudy)*log2(P(weather=cloudy)) - P(weather=rainy)*log2(P(weather=rainy))$\n",
    "\n",
    "$H(weather) = - 5/14 * log2(5/14) - 4/14 * log2(4/14) - 5/14 * log2(5/14) = 1.5774062828523454 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(playOut) = 0.9402859586706311\n",
      "H(weather) = 1.5774062828523454\n"
     ]
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Results :\n",
    "# H(playOut) = 0.9402859586706311\n",
    "# H(weather) = 1.5774062828523454\n",
    "#---------------------------------------------------------------------\n",
    "print(\"H(playOut) =\", H(Y_playOut))\n",
    "print(\"H(weather) =\", H(X_playOut[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Splitting of a set\n",
    "\n",
    "Here we try to split the list of predictions (classes) according to the values of an attribute (characteristic, column) into sub-lists, Let : \n",
    "- $S$ : be the list of values to be divided (Represented as a vector, since we have to use the order of the elements)\n",
    "- $A$ : the set of values of an attribute (characteristic, column). It is a vector aligned with S; i.e., each element of A has a respective element of S.\n",
    "- $val$ : be the value on which we base our split.\n",
    "\n",
    "This can be formulated as follows:\n",
    "\n",
    "$$S_{A, val} = \\{v_i \\in S  / i \\in \\{j / a_j \\in A \\text{ et } a_j = val\\}\\}$$\n",
    "\n",
    "Or simply :\n",
    "\n",
    "$$S_{A,val} = \\{s_i \\in S / a_i \\in A \\wedge a_i = val\\}\\}$$\n",
    "\n",
    "For example, let's say :\n",
    "- $S$ is the set of \"**PlayOut**\" classes e.g., [yes, no, yes , yes , ...]\n",
    "- $A$ is the set of characteristic values \"**weather**\" e.g, [sunny, cloudy, ...]\n",
    "- $val$ is the value \"**sunny**\" \n",
    "\n",
    "Therefore, the subset of \"**PlayOut**\" where (**weather = \"sunny\"**) contains 3 \"no\"s and 2 \"yes\"s Classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the function code\n",
    "# it must return a subset of S\n",
    "# where the respective values in A are equal to val\n",
    "\n",
    "def split_ID3(S: np.ndarray, A: np.ndarray, val: str) -> np.ndarray:\n",
    "    '''\n",
    "    Input :\n",
    "        S : numpu.ndarray, the list of values to be divided.\n",
    "        A : numpu.ndarray, the list of the characteristic values.\n",
    "        val : string value, on which we base our split.\n",
    "    Output : \n",
    "        subset : numpy.ndarray, a subset of S where the respective values in A are equal to val\n",
    "    '''\n",
    "    ### BEGIN : Write your code here\n",
    "\n",
    "    subset = []\n",
    "\n",
    "    for i in range(len(A)):\n",
    "        if A[i] == val:\n",
    "            subset.append(S[i])\n",
    "\n",
    "\n",
    "    ### END\n",
    "    return np.array(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['no', 'no', 'no', 'yes', 'yes'], dtype='<U3')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Results : \n",
    "# array(['no', 'no', 'no', 'yes', 'yes'], dtype=object)\n",
    "#---------------------------------------------------------------------\n",
    "split_ID3(Y_playOut, X_playOut[:,0], \"sunny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we split the traget column \"PlayOut\" (Y_playOut) into three subsets according to the values of the \"weather\" column. \n",
    "- For each subset, we calculate the entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Y_sunny) = 0.9709505944546686\n",
      "H(Y_cloudy)= 0.0\n",
      "H(Y_rainy) = 0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "Y_sunny = split_ID3(Y_playOut, X_playOut[:,0], \"sunny\") \n",
    "# PalyOut: 2 yes, 3 no\n",
    "# H(S_sunny) = - 2/5 * log2(2/5) - 3/5 * log2(3/5) = 0.9709505944546686\n",
    "\n",
    "Y_cloudy = split_ID3(Y_playOut, X_playOut[:,0], \"cloudy\") # jouer: 4 oui, 0 non\n",
    "# P(S_cloudy) = - 4/4 * log2(4/4) - 0/4 * log2(0/4) = 0.0\n",
    "\n",
    "Y_rainy = split_ID3(Y_playOut, X_playOut[:,0], \"rainy\") # jouer: 3 oui, 2 non\n",
    "# P(S_rainy) = - 3/5 * log2(3/5) - 2/5 * log2(2/5) = 0.971\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Results : \n",
    "# H(Y_sunny) = 0.9709505944546686\n",
    "# H(Y_cloudy)= 0.0\n",
    "# H(Y_rainy) = 0.9709505944546686\n",
    "#---------------------------------------------------------------------\n",
    "print(\"H(Y_sunny) =\", H(Y_sunny))\n",
    "print(\"H(Y_cloudy)=\", H(Y_cloudy))\n",
    "print(\"H(Y_rainy) =\", H(Y_rainy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the entropy of the set (weather = cloudy) is 0 \n",
    "\n",
    "**Question (Q1)** : \n",
    "- What does this mean ?\n",
    "- Do we need to divide this set using another characteristic ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer (A1) :**\n",
    "[ Write your answer here ]\n",
    "\n",
    "- It means that there is not diverse data in the set, which means that it is leaf node in decisoin tree and there is no need to calculate further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5 Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Information Gain (IG) is the difference between the entropy before and after the division of a set $S$ based on the attribute $A$. \n",
    "\n",
    "In other words, how much uncertainty in $S$ has been reduced after splitting it using the attribute $A$.\n",
    "\n",
    "Given : \n",
    "- $S$: a set (in our case, the set of classes) \n",
    "- $A$: set of values of an attribute (characteristic, column) \n",
    "- $V$ : the set of the different values of the attribute $A$\n",
    "- $P(v/A)$ : the likelihood of occurrence of the value $v$ in $A$\n",
    "- $S_{A, v}$ : subset of $S$ where the values of $A$ equal $v$  (using the previous function (split_ID3)\n",
    "\n",
    "The information gain is calculated as follows: \n",
    "\n",
    "$$IG(S, A) = H(S) - \\sum_{v \\in V} p(v/A) H(S_{A, v})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Complete the information gain function code\n",
    "# It must return the information gain and the entropy \n",
    "# Entropy is returned so that it is not recalculated later \n",
    "\n",
    "def IG(S: np.ndarray, A: np.ndarray) -> Tuple[float, float]:\n",
    "    '''\n",
    "    Inputs :\n",
    "        S: np.ndarray, array of the target variable (classes)\n",
    "        A: np.ndarray, array of values of an attribute (characteristic, column) to calculate the IG on.\n",
    "    Outputs :\n",
    "        ig_A : the information gain score we obtain if we split S based on A attributes\n",
    "        ig_global : The global information gain entropy\n",
    "    '''\n",
    "    vals = np.unique(A) # Get unique value of the array \"A\"\n",
    "    ig_global = H(S)    # Calculate the entropy of the array S before the splits \n",
    "    ig_A = ig_global    # Set ig_A to the maximum \n",
    "\n",
    "    for val in vals:\n",
    "        print(H(split_ID3(S, A, val)))\n",
    "\n",
    "    ### BEGIN : Write your code here\n",
    "    # Hint : Apply the formula using a loop\n",
    "        \n",
    "    ### END\n",
    "    \n",
    "    return ig_A, ig_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hand Calculation :** \n",
    "\n",
    "- $ IG(PlayOut, Weather) = H(Y_playOut) - P(Weather = sunny /A) * H(S_sunny)  \n",
    "                                        - P(temps = nuageux   /A) * H(S_nuageux) \n",
    "                                        - P(temps = pluvieux  /A) * H(S_pluvieux) $\n",
    "                                        \n",
    "\n",
    "- $ IG(PlayOut, Weather) = 0.9402859586706311 - P(temps = ensoleilÃ©)* (0.9709505944546686) \n",
    "                                              - P(temps = nuageux)*(0.0) \n",
    "                                              - P(temps = pluvieux) *(0.9709505944546686) $\n",
    "\n",
    "\n",
    "- $ IG(PlayOut, Weather) = 0.2467498197744391 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9709505944546686\n",
      "0.9709505944546686\n",
      "IG(PlayOut, weather) :  (np.float64(0.9402859586706311), np.float64(0.9402859586706311))\n",
      "0.9182958340544896\n",
      "1.0\n",
      "0.8112781244591328\n",
      "IG(PlayOut, temperature) :  (np.float64(0.9402859586706311), np.float64(0.9402859586706311))\n",
      "0.5916727785823275\n",
      "0.9852281360342515\n",
      "IG(PlayOut, humidity) :  (np.float64(0.9402859586706311), np.float64(0.9402859586706311))\n",
      "0.8112781244591328\n",
      "1.0\n",
      "IG(PlayOut, wind) :  (np.float64(0.9402859586706311), np.float64(0.9402859586706311))\n"
     ]
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Results : \n",
    "# IG(Y_playOut, Weather) :  (0.24674981977443933, 0.9402859586706311)\n",
    "# IG(Y_playOut, temperature) :  (0.02922256565895487, 0.9402859586706311)\n",
    "# IG(Y_playOut, humidity) :  (0.15183550136234164, 0.9402859586706311)\n",
    "# IG(Y_playOut, wind) :  (0.048127030408269544, 0.9402859586706311)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "print(\"IG(PlayOut, weather) : \",IG(Y_playOut, X_playOut[:, 0]))\n",
    "print(\"IG(PlayOut, temperature) : \",IG(Y_playOut, X_playOut[:, 1]))\n",
    "print(\"IG(PlayOut, humidity) : \",IG(Y_playOut, X_playOut[:, 2]))\n",
    "print(\"IG(PlayOut, wind) : \",IG(Y_playOut, X_playOut[:, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question (Q2) :**\n",
    "- What do you think is the ideal feature we should use to divide the first node of our decision tree?\n",
    "- Why ? (Please provied an explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer (A2) :**\n",
    "[ Write your answer here ]\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6 Selection of the ID3 split feature \n",
    "- The goal here is to code a function that will help us to find the relevant Splitting feature for each given node level.\n",
    "- To do that, we should look for the feature that maximises IG :\n",
    "$$ f\\_index = \\arg\\max_i IG(Y, X_i)$$\n",
    "- The function must return the index of the relevant feature $f\\_index$, the IG score of the split using f_index and the entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the following function code \n",
    "# splitting_feature_ID3(X, Y) : seeks the most relevant feature to split Y.\n",
    "# it must also return the Information Gain (IG) and H(entropy) of this relevant feature.\n",
    "\n",
    "def splitting_feature_ID3(X: np.ndarray, Y: np.ndarray) -> Tuple[int, float, float]: \n",
    "    '''\n",
    "    This function seeks the most relevant feature to split Y.\n",
    "    Returns the IG and H of selected feature. \n",
    "    mathematical formula : Max(IG(Y,X[i])) => return the index i (argmax)\n",
    "    '''\n",
    "    # create an empty list to get calculate all the Information gain (IG) scores for every feature in X\n",
    "    IG_vector = []\n",
    "    # loop over the columns of X and calculate the IG score\n",
    "    for i in range(X.shape[1]):\n",
    "        # we append respectively the scores to the IG_vector list of IG scores \n",
    "        IG_vector.append(IG(Y, X[:, i]))\n",
    "    \n",
    "    ### BEGIN : Write your code here\n",
    "    \n",
    "    # Get the index of the relevant feature \n",
    "    featureId = None\n",
    "    \n",
    "    # Calculate the information gain using this selected feature \n",
    "    ig        = None\n",
    "    \n",
    "    # Calculate the global entropy\n",
    "    h         = None\n",
    "    \n",
    "    ### END\n",
    "    \n",
    "    return featureId, ig, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# (0, 0.24674981977443933, 0.9402859586706311)\n",
    "#---------------------------------------------------------------------\n",
    "splitting_feature_ID3(X_playOut, Y_playOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.7 Creation of the decision tree\n",
    "*Note : Nothing to code or analyse here.*\n",
    "- We create a structure for the tree (Node class)\n",
    "- We implement a recursive function to create a tree from a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    '''\n",
    "    # Node Class : A class to hold the Node information and the list of its children.\n",
    "    '''\n",
    "    nbr = 0 # RootID\n",
    "    \n",
    "    def __init__(self, featureId, ig, h, depth): \n",
    "        self.featureId = featureId    # The featureId of the feature we used for spliting to get this node  \n",
    "        self.ig        = ig           # The Information Gain of the spletting node\n",
    "        self.h         = h            # the entropy H\n",
    "        self.depth     = depth        # the depth of the node\n",
    "        self.children  = {}           # Node children, we store them in a dictionary of type Node \n",
    "        self.cls       = \"\"           # the class if this node is final (if there are no children)\n",
    "        self.indent    = \"    \"       # indentation when generating the pseaudo code\n",
    "    \n",
    "    def __str__(self):\n",
    "        '''\n",
    "        This function (parser) is to transform the node to a string.\n",
    "        Here we have redefined this function to write the tree as a pseaudo algorithm.\n",
    "        '''\n",
    "        indent = self.indent * self.depth               # indentation: aesthetics\n",
    "        if (len(self.children)==0):                     # IF there are no children, the node is terminal (leaf) \n",
    "            return indent + 'Y IS \"' + self.cls + '\"\\n' # => we print the class\n",
    "        res = \"\"                                        # ELSE, we loop over the children nodes and print IFs ... ELSE\n",
    "        \n",
    "        for childNode in self.children:\n",
    "            res += indent + 'IF X[' + str(self.featureId) + '] IS \"' + str(childNode) + '\" THEN\\n' + str(self.children[childNode])\n",
    "            \n",
    "        return res\n",
    "    \n",
    "    def predict(self, x: List[str]) -> str: \n",
    "        '''\n",
    "        Prediction Function : Given and instnace X it return the predicted target class Y'\n",
    "        '''\n",
    "        # IF the node is final, it returns its class\n",
    "        if (len(self.children)==0):                     \n",
    "            return self.cls\n",
    "        # IF the value of the respective column at this node does not belong to the set of expected values, we return np.nan        \n",
    "        if x[self.featureId] not in self.children:      \n",
    "            return np.nan                               \n",
    "        # Else, we do a recursive call and run the instance over the decision tree \n",
    "        return self.children[x[self.featureId]].predict(x)\n",
    "    \n",
    "    def graphviz(self): \n",
    "        '''\n",
    "        This functiongenerate the Pseudocode for graphviz\n",
    "        '''\n",
    "        nid = 'N' + str(Node.nbr)\n",
    "        Node.nbr += 1\n",
    "        # If the node is a final one,\n",
    "        if (len(self.children)==0):\n",
    "            return nid, nid + '[label=\"' + self.cls + '\" shape=ellipse];\\n'\n",
    "        # Else if there are children nodes, we loop over them and print IF ... THEN .. structure\n",
    "        res = nid + '[label=\"X[' + str(self.featureId) + ']\\\\n'\n",
    "        res += 'H = ' + str(self.h) + '\\\\n'\n",
    "        res += 'IG = ' + str(self.ig) + '\"];\\n'\n",
    "        for value in self.children:\n",
    "            vid, code = self.children[value].graphviz()\n",
    "            res += code\n",
    "            res += nid + ' -> ' + vid + ' [label=\"' + value + '\"];\\n'\n",
    "        return nid, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ID3(X, Y, depth=0, pruning=False): \n",
    "    '''\n",
    "    Training function : Learn and create the decision tree from a set of X (features) and Y(Target/Labels)\n",
    "    '''\n",
    "    featureId, ig, h = splitting_feature_ID3(X, Y)     # Find the best feature of X to split Y\n",
    "    node = Node(featureId, ig, h, depth)           # Create a node                              \n",
    "    \n",
    "    if h == 0.0:                                   # If the entropy is 0 then the node is a leaf\n",
    "        node.cls = Y[0]                            # the class of the node\n",
    "        return node                                # return the node\n",
    "     \n",
    "    if depth > 3 and pruning:                      # There is no pruning in ID3, but it must be enabled  \n",
    "        node.cls = max(set(Y))                     # To avoid the problem of max recursivity\n",
    "        return node                                # return the node\n",
    "    \n",
    "                                                   # Else, if the node is not a leaf, we create these children\n",
    "    depth += 1                                     # the depth of its children\n",
    "                                                   # the children are created from the unique values of the selected relevant feature\n",
    "    for val in np.unique(X[:, featureId]):\n",
    "        msk = X[:, featureId] == val               # These three lines are to retrieve the subsets X_val, Y_val\n",
    "        X_val = X[msk]                             # corresponding to a value of the selected relevant feature\n",
    "        Y_val = Y[msk]                             # We do the same operation on the set (Y_val) in a recursive way\n",
    "        children = train_ID3(X_val, Y_val, depth, pruning) \n",
    "        node.children[val] = children                    \n",
    "                                                  # We assign the created node indexed by the value of the selected relevant feature \n",
    "                                                  # to the children of the current node\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playOut_Tree = train_ID3(X_playOut, Y_playOut)\n",
    "\n",
    "print(\"The Decision Tree ID3 Pseudocode : \\n\")\n",
    "print(playOut_Tree)\n",
    "\n",
    "# Test our model on a sample (new instance) : [\"cloudy\", \"no_temperature\", \"no_humidity\", \"no\"]\n",
    "x_test = [\"cloudy\", \"no_temperature\", \"no_humidity\", \"no\"]\n",
    "pred = playOut_Tree.predict(x_test)\n",
    "print(\"X = \",x_test,\" | Prediction : Y'=\",pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.8 Bundling all functions together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *PS : Nothing to code here*\n",
    "- There is a question to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(object): \n",
    "    \n",
    "    def train(self, X, Y, X_names=[], Y_name=\"\", pruning=False):\n",
    "        self.tree = train_ID3(X, Y, pruning=pruning)\n",
    "        pseudoCode  = str(self.tree)\n",
    "        if len(Y_name) > 0: \n",
    "            pseudoCode = pseudoCode.replace(\"Y\", Y_name)\n",
    "        for i in range(len(X_names)): \n",
    "            pseudoCode = pseudoCode.replace(\"X[\" + str(i) + \"]\", X_names[i])\n",
    "        self.pseudoCode = pseudoCode\n",
    "        self.X_names = X_names\n",
    "    \n",
    "    def predict(self, X): \n",
    "        predictions = []\n",
    "        for i in range(len(X)): \n",
    "            predictions.append(self.tree.predict(X[i, :]))\n",
    "        return predictions\n",
    "    \n",
    "    def graphviz(self): \n",
    "        nid, pseudoCode = self.tree.graphviz()\n",
    "        res = \"digraph Tree {\\n\"\n",
    "        res += \"node [shape=box] ;\"\n",
    "        for i in range(len(self.X_names)): \n",
    "            pseudoCode = pseudoCode.replace(\"X[\" + str(i) + \"]\", self.X_names[i])\n",
    "        res += pseudoCode\n",
    "        res += \"}\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate and train the classifier\n",
    "id3_classifier  = ID3()\n",
    "\n",
    "# Training\n",
    "id3_classifier.train(X_playOut, Y_playOut, X_names=[\"weather\", \"temperature\", \"humidity\", \"wind\"], Y_name=\"PlayOut\")\n",
    "\n",
    "# visualization of the pseudo-code\n",
    "print(\"ID3 Classifier pseudo-code :\\n\\n\",id3_classifier.pseudoCode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :** In the code above, we can see that the decision tree does not take into account the \"temperature\" feature\n",
    "\n",
    "**Question (Q3):**\n",
    " - What can you say about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer (A3) :**\n",
    "[ Write your answer here ]\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graphviz Installation Setps \n",
    "\n",
    "##### For Windows :\n",
    "\n",
    "1. Go to the [Graphviz website](http://www.graphviz.org)  and download and install to your computer (do NOT need to install for all users).\n",
    "2. Download and install Anaconda3.5 from the Continuum website. (This should be already installed)\n",
    "3. Add Graphviz to the environment variable \"Path\":\n",
    "    - Go to Computer > Properties > Advanced system settings > Environment Variables and then find \"Path\" in the system variables box. Click on Path and click edit.\n",
    "    - Append ;C:\\Program Files (x86)\\Graphviz2.38\\bin to the end of the many paths that are already present in Path. Note, the path to Graphviz may be different for you so make sure to put the correct path. The folder \"bin\" should have many files including the dot.exe application.\n",
    "    - To check the install go to the command prompt and enter: dot -V this should return the version of Graphviz installed. For example, dot - graphviz version 2.38.0. If this does not work, enter set and look for the Graphviz path.\n",
    "4. Go to the Anaconda command prompt and enter: pip install graphviz\n",
    "5. Restart jupyter instance or launch it if not already open.\n",
    "\n",
    "Source : https://stackoverflow.com/questions/36869258/how-to-use-graphviz-with-anaconda-spyder\n",
    "\n",
    "##### For Linux/Mac users :\n",
    "\n",
    "- If you use the conda package manager, the graphviz binaries and the python package can be installed with the following command :\n",
    "\n",
    "> conda install python-graphviz\n",
    "\n",
    "- Alternatively binaries for graphviz can be downloaded from the graphviz project homepage, and the Python wrapper installed from pypi with : \n",
    "\n",
    "> pip install graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a visualisation of the graph.\n",
    "# If it doesn't work, it isn't a big deal (you can move to the next cell).\n",
    "try:\n",
    "    from IPython.display import SVG\n",
    "    from graphviz import Source\n",
    "    from IPython.display import display\n",
    "    \n",
    "    graph = Source(id3_classifier.graphviz())\n",
    "    display(SVG(graph.pipe(format='svg')))\n",
    "\n",
    "except ImportError:\n",
    "    print(\"graphviz must be installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 CART  (Classification and Regression Trees)[[Breiman et al. , 1984](Refrence)]\n",
    "- Here we will implement the CART algorithm for classification with numerical features. \n",
    "- For that, we will use the dataset \"PlayOut_num.csv\" (numerical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Reading and preparation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset \"PlayOut.csv\" using pandas.read_csv\n",
    "df_playOut_num = pd.read_csv(\"data/PlayOut_num.csv\")\n",
    "# Print the DataFrame df_playOut\n",
    "df_playOut_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to separate data into : (1) inputs noted X, and (2) outputs(labels, classes) noted Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first columns that represents the features : ['weather','temperature','humidity','wind']\n",
    "# Not including the last column\n",
    "X_NumPlayOut = df_playOut_num.iloc[:, :-1].values\n",
    "\n",
    "# Get the target column : the last column that contains labels (classes) \n",
    "Y_NumPlayOut = df_playOut_num.iloc[:,-1].values\n",
    "\n",
    "# Print X\n",
    "print(\"X_playOut : \\n\",X_NumPlayOut)\n",
    "\n",
    "# Print Y\n",
    "print(\"\\nY_playOut : \\n\",Y_NumPlayOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Probability Function\n",
    "Already implemented in ID3 Section\n",
    "\n",
    "#### 2.2.3 Gini index\n",
    "\n",
    "- In the classification case, CART uses the Gini index of diversity to measure the classification error.\n",
    "- An index of 0 represents the best split. \n",
    "\n",
    "Given : \n",
    "- $S$ list of values  \n",
    "- $V$ set of unique values of $S$ (vocabulary)\n",
    "\n",
    "The diversity index $Gini(S)$ is calculated as follows: \n",
    "\n",
    "$$Gini(S) = \\sum\\limits_{v \\in V} p(v/S) (1-p(v/S)) = 1 - \\sum\\limits_{v \\in V} p(v/S)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the Gini function code\n",
    "def Gini(S: np.ndarray) -> float:  \n",
    "    V = np.unique(S)\n",
    "    gini = 1\n",
    "    ### BEGIN : Write your code here\n",
    "\n",
    "    ### END \n",
    "    return gini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# Gini(Y)= 0.4591836734693877\n",
    "#---------------------------------------------------------------------\n",
    "print(\"Gini(Y)=\",Gini(Y_NumPlayOut))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Splitting of a set\n",
    "- *NP : Nothing to code here*\n",
    "\n",
    "As CART (the decision tree algoritm) generates **binary trees**, we need to code a spletting function that splits the list of predictions (classes) $Y$ according to a given value $v$ of an attribute (feature, column) $A$ into two lists:\n",
    "- $Y_L$: (left List) a list containing the elements of $Y$ where $a \\in A$ : $a > v$\n",
    "- $Y_R$: (Right List) a list containing the elements of $Y$ where $a \\in A$ : $a \\le v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the case A is a nominal feature\n",
    "def split_nom_bin(S, A, value):\n",
    "    msk     = (A==value)       # Ceate a mask to filter the numpy array S, and return \n",
    "    S_left  = S[msk]           # the left set : S_{A == values}\n",
    "    S_right = S[~msk]          # the right set : S_{A != values} \n",
    "    return S_left, S_right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the case A is numerical feature\n",
    "def split_num_bin(S, A, value):\n",
    "    msk = A > value            # Ceate a mask to filter the numpy array S\n",
    "    S_left  = S[msk]           # the left set S_{A > values} \n",
    "    S_right = S[~msk]          # the right set S_{A <= values} \n",
    "    return S_left, S_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_CART(S, A, value) :\n",
    "    '''\n",
    "    The function checks whether the value is numeric or not.\n",
    "    It calls the two previous functions depending on the type of the feature\n",
    "    '''\n",
    "    try:\n",
    "        val = float(value)\n",
    "        return split_num_bin(S, A, val)\n",
    "    except ValueError:\n",
    "        return split_nom_bin(S, A, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST 01 (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Results :\n",
    "# --------------------------------------------------------------------\n",
    "# Letf split of Y based on \"temperature\" (2nd column) feature : \n",
    "#  ['no' 'no' 'yes' 'yes' 'no' 'yes' 'yes' 'yes' 'yes' 'yes' 'no']\n",
    "# Righ split of Y based on \"temperature\" (2nd column) feature :\n",
    "#  ['yes' 'no' 'yes']\n",
    "#---------------------------------------------------------------------\n",
    "# Test with numerical variables\n",
    "Y_NumPlayOut_letf, Y_NumPlayOut_right = split_CART(Y_NumPlayOut, X_NumPlayOut[:,1], 20)\n",
    "print(\"Letf split of Y based on \\\"temperature\\\" (2nd column) feature : \\n\", Y_NumPlayOut_letf)\n",
    "print(\"Righ split of Y based on \\\"temperature\\\" (2nd column) feature :\\n\", Y_NumPlayOut_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST 02 (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Results :\n",
    "# --------------------------------------------------------------------\n",
    "# Letf split of Y based on \"weather\" (2nd column) feature : \n",
    "#  ['no' 'no' 'no' 'yes' 'yes']\n",
    "# Righ split of Y based on \"weather\" (2nd column) feature :\n",
    "#  ['yes' 'yes' 'yes' 'no' 'yes' 'yes' 'yes' 'yes' 'no']\n",
    "#---------------------------------------------------------------------\n",
    "# Test with nominal variables\n",
    "Y_NumPlayOut_letf, Y_NumPlayOut_right = split_CART(Y_NumPlayOut, X_NumPlayOut[:, 0], \"sunny\")\n",
    "print(\"Letf split of Y based on \\\"weather\\\" (2nd column) feature : \\n\", Y_NumPlayOut_letf)\n",
    "print(\"Righ split of Y based on \\\"weather\\\" (2nd column) feature :\\n\", Y_NumPlayOut_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5 The Gini score of the split\n",
    "\n",
    "\n",
    "Given :\n",
    "\n",
    "- $S$: a list (in our case, the list of classes labels) \n",
    "- $S_L, S_R$ :  **left** and **right** subsets after the split\n",
    "- $|S| = |S_L| + |S_R|$\n",
    "\n",
    "The Gini score of the split is calculated as follow : \n",
    "\n",
    "$$Gini_{split}(S_L, S_R) = \\frac{|S_L|}{|S|} Gini(S_L) + \\frac{|S_R|}{|S|} Gini(S_R)$$\n",
    "\n",
    "Our goal is to try to minimize $Gini_{split}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the code of the function below to calculate the gini socre of the split\n",
    "\n",
    "def split_Gini(S_L: np.ndarray, S_R: np.ndarray) -> float:  \n",
    "    SL_len = len(S_L)              # Get the cardinality (length) of the left subset S_L\n",
    "    SR_len = len(S_R)              # Get the cardinality (length) of the Right subset S_R\n",
    "    S_len = float(SL_len + SR_len) # Get the cardinality (length) of the set S\n",
    "    \n",
    "    ### BEGIN : Write your code here\n",
    "    # Hint : follow the formula \n",
    "    \n",
    "    ### END\n",
    "    return gini_SL_SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Results : \n",
    "# Gini_split(S_L_numerical, S_R_numerical) = 0.4588744588744589\n",
    "# Gini_split(S_L_nominal, S_R_nominal) = 0.4666666666666667\n",
    "#---------------------------------------------------------------------\n",
    "# There is two way to call the split_Gini function\n",
    "\n",
    "# 1st way :\n",
    "# Call split_CART  (in this case we are using a numerical feature to split the predictions)\n",
    "# Save the new subsets in \"S_L_numerical\" and \"S_R_numerical\"\n",
    "S_L_numerical, S_R_numerical = split_CART(Y_NumPlayOut, X_NumPlayOut[:, 1], 20)\n",
    "# then call split_Gini \n",
    "S_gini_num = split_Gini(S_L_numerical, S_R_numerical)\n",
    "print(\"Gini_split(S_L_numerical, S_R_numerical) =\", S_gini_num)\n",
    "\n",
    "# 2nd way : Do a nesting function call with the Starred Expression in Python \"func(*args)\"\n",
    "# here we calculate the gini score for the split with the nominal feature \"Weather\" \n",
    "S_gini_nom = split_Gini(*split_CART(Y_NumPlayOut, X_NumPlayOut[:, 0], \"sunny\"))\n",
    "print(\"Gini_split(S_L_nominal, S_R_nominal) =\",S_gini_nom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.6 Selection of the CART splitting feature \n",
    "- *PS: Nothing to code here*\n",
    "\n",
    "**The Pseudo-code :**\n",
    "\n",
    "- For each feature $X_j$  \n",
    "   - For each unique value $v$ in $X_j$\n",
    "       1. Divide $Y$ based on the value of $v$ and those of $X_j$\n",
    "       2. Calculate the Gini score of this split \n",
    "       3. Keep the index **selectedFeatureIndex** of the feature that minimizes Gini\n",
    "       4. Keep Gini minimal **selectedFeature_gini**\n",
    "       5. Keep the feature split value **selected_val**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete by following the pseudo-code\n",
    "# This function searches for the feature and its value that is the most relevant for spliting  Y \n",
    "\n",
    "# it must return:\n",
    "    # selectedFeatureIndex: the index of the selected feature in X\n",
    "    # selected_val: the split value (the value can be string or numeric)\n",
    "    # selected_gini: the split gini score \n",
    "    \n",
    "def splitting_feature_CART(X: np.ndarray, Y: np.ndarray) -> Tuple[int, float, float]:\n",
    "    # Create two empty lists for saving results of diffrent splits\n",
    "    # So that we could find in later step the one tha minimize the Gini_split function\n",
    "    Gini_vector_Global = []\n",
    "    val_vector_Global = []\n",
    "    # Loop over the features columns in X\n",
    "    for i in range(X.shape[1]):           \n",
    "        Gini_vector_local = []\n",
    "        val_vector_local = []\n",
    "        # Get the column of the selcted feature i \n",
    "        A = X[:,i]\n",
    "        # loop over the unique values of the selected feature (do that for each feature in X) \n",
    "        Vals = np.unique(A)               \n",
    "        for val in Vals:\n",
    "            # the index of the array represent the index of the feature \n",
    "            # Save the value \"val\" of the feature\n",
    "            val_vector_local.append(val)\n",
    "            # Split over the entire selected feature with given value \"val\", and save the Gini_score score\n",
    "            Gini_vector_local.append(split_Gini(*split_CART(Y,A,val))) \n",
    "            \n",
    "        # Minimize Gini Score for this feature\n",
    "        # Get and save the min Gini value \n",
    "        Min = np.amin(Gini_vector_local)\n",
    "        Gini_vector_Global.append(Min)\n",
    "        # Get and save the value that minimize Gini value for this feature\n",
    "        v   = val_vector_local[np.argmin(Gini_vector_local)]\n",
    "        val_vector_Global.append(v)\n",
    "        \n",
    "    # minimiser gini score across all the features  \n",
    "    selectedFeatureIndex = np.argmin(np.array(Gini_vector_Global))\n",
    "    selectedFeature_gini= min(Gini_vector_Global)\n",
    "    selected_val = val_vector_Global[selectedFeatureIndex]\n",
    "    \n",
    "    return selectedFeatureIndex, selectedFeature_gini, selected_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Results : \n",
    "# selected Feature Index :  0\n",
    "# Gini Spliting Score :  0.35714285714285715\n",
    "# Spliting Value :  cloudy\n",
    "#---------------------------------------------------------------------\n",
    "selectedFeatureIndex, gini_score, selectedFeature_val = splitting_feature_CART(X_NumPlayOut, Y_NumPlayOut)\n",
    "print(\"selected Feature Index : \", selectedFeatureIndex)\n",
    "print(\"Gini Spliting Score : \", gini_score)\n",
    "print(\"Spliting Value : \", selectedFeature_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.7 Creation of the decision tree CART\n",
    "*PS : Nothing to code here*\n",
    "- CART uses the pre-splitting method referred to by the stopping condition. \n",
    "- The most used criterion to stop the splitting is the minimal number of samples in a node. If we reach this number, we do not divide anymore and we consider the node as a leaf with the dominant class as output class in case of ranking, or the average of the outputs in case of regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinNode(object): \n",
    "    '''\n",
    "    BinNode Class : A class to hold the Binary Node information and the list of its children (two children).\n",
    "    '''\n",
    "    nb = 0\n",
    "    \n",
    "    def __init__(self, featureId, val, gini, depth):\n",
    "        self.featureId = featureId # The index in X of the feature we used for spliting to get this node  \n",
    "        self.val = val             # The split value\n",
    "        self.gini = gini           # Gini Score of the split\n",
    "        self.depth = depth         # the depth of the node\n",
    "        self.children = []         # Node children, as we have two children (S_L, S_R) we store them in a list of type Node \n",
    "        self.cls = \"\"              # the class if this the node is final (if there are no children)\n",
    "        self.indent = \"    \"       # indentation when generating the pseaudo code\n",
    "        # save the type of the feature \n",
    "        try: # case it is numerical \n",
    "            val2 = float(val)\n",
    "            self.is_num = True\n",
    "        except ValueError: # case it's a nominal (string)\n",
    "            self.is_num = False\n",
    "\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        This function (parser) is to transform the node to a string.\n",
    "        Here we have redefined this function to write the tree as a pseaudo algorithm.\n",
    "        '''\n",
    "        # indentation: aesthetics\n",
    "        indent = self.indent * self.depth \n",
    "        \n",
    "        if (len(self.children)==0):\n",
    "            return indent + 'Y IS \"' + self.cls + '\"\\n'\n",
    "        if (self.is_num): \n",
    "            prefix = ' > '\n",
    "            suffix = ''\n",
    "        else:\n",
    "            prefix = ' IS \"'\n",
    "            suffix = '\"'\n",
    "        \n",
    "        res = \"\"\n",
    "        res += indent + 'IF X[' + str(self.featureId) + '] ' + prefix + str(self.val) + suffix + ' THEN \\n' + str(self.children[0])\n",
    "        res += indent + 'ELSE \\n' + str(self.children[1])\n",
    "        return res\n",
    "\n",
    "    def predict(self, x): \n",
    "        '''\n",
    "        Prediction Function : Given and instnace X it return the predicted target class Y'\n",
    "        '''\n",
    "        # IF the node is final, it returns its class\n",
    "        if (len(self.children)==0):\n",
    "            return self.cls\n",
    "        # ELSE\n",
    "        # in case it's numerical feature\n",
    "        if self.is_num: \n",
    "            if x[self.featureId] > self.val:\n",
    "                return self.children[0].predict(x)\n",
    "            return self.children[1].predict(x)\n",
    "        # in case it's nominal feature (string)\n",
    "        if x[self.featureId] == self.val:\n",
    "            return self.children[0].predict(x)\n",
    "        return self.children[1].predict(x)\n",
    "\n",
    "\n",
    "    def graphviz(self): \n",
    "        '''\n",
    "        This function generate the Pseudocode for graphviz\n",
    "        '''\n",
    "        nid = 'N' + str(BinNode.nb)\n",
    "        BinNode.nb += 1\n",
    "        # If the node is a final one,\n",
    "        if (len(self.children)==0):\n",
    "            return nid, nid + '[label=\"' + self.cls + '\" shape=ellipse];\\n'\n",
    "        # Else if there are children nodes, we loop over them and print IF ... THEN .. structure\n",
    "        if self.is_num: \n",
    "            prefix = '] > '\n",
    "        else:\n",
    "            prefix = '] = '\n",
    "        res = nid + '[label=\"X[' + str(self.featureId) + prefix + str(self.val) + '\\\\n'\n",
    "        res += 'Gini = ' + str(self.gini) + '\"];\\n'\n",
    "        vid_L, code_L = self.children[0].graphviz()\n",
    "        vid_R, code_R = self.children[1].graphviz()\n",
    "        \n",
    "        res += code_L + code_R\n",
    "        res += nid + ' -> ' + vid_L + ' [label=\"Vrai\"];\\n'\n",
    "        res += nid + ' -> ' + vid_R + ' [label=\"Faux\"];\\n'\n",
    "        return nid, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CART(X, Y, depth=0, nb_max=3): \n",
    "    '''\n",
    "    Training function : Learn and create the decision tree from a set of X (features) and Y(Target/Labels)\n",
    "    '''\n",
    "    f_index, f_gini, f_val = splitting_feature_CART(X, Y) # Find the best relevant feature of X to split Y\n",
    "    node = BinNode(f_index, f_val, f_gini, depth)         # Create a node\n",
    "    \n",
    "    if (f_gini == 0.0) or (len(Y) <= nb_max):             # Pruning\n",
    "        node.cls = max(set(Y))                            # the class of the node (most frequent value)\n",
    "        return node                                       # return the node \n",
    "        \n",
    "    # Otherwise, if the node is not terminal, we create its children nodes\n",
    "    depth += 1 \n",
    "    \n",
    "    # creation of the two children nodes\n",
    "    try: \n",
    "        val2 = float(f_val)\n",
    "        msk = X[:, f_index] > val2\n",
    "    except ValueError:\n",
    "        msk = (X[:, f_index]==f_val)\n",
    "    \n",
    "    X_L = X[msk]\n",
    "    Y_L = Y[msk]\n",
    "    children_L = train_CART(X_L, Y_L, depth, nb_max)\n",
    "    \n",
    "    X_R = X[~msk]\n",
    "    Y_R = Y[~msk]\n",
    "    children_R = train_CART(X_R, Y_R, depth, nb_max)\n",
    "    node.children.append(children_L)\n",
    "    node.children.append(children_R)\n",
    "    \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playOut_Tree_CART = train_CART(X_NumPlayOut, Y_NumPlayOut)\n",
    "\n",
    "print(\"The Decision Tree CART Pseudocode : \\n\")\n",
    "print(playOut_Tree_CART)\n",
    "\n",
    "# Test our model on a sample (new instance) : [\"cloudy\", \"no_temperature\", \"no_humidity\", \"no\"]\n",
    "x_test = [\"cloudy\", \"no_temperature\", \"no_humidity\", \"no\"]\n",
    "pred = playOut_Tree_CART.predict(x_test)\n",
    "print(\"X = \",x_test,\" | Prediction : Y'=\",pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.8 Bundling all functions together\n",
    "Nothing to program here. There is a question to answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART(object): \n",
    "    \n",
    "    def train(self, X, Y, X_names=[], Y_name=\"\", nb_max=3):\n",
    "        self.tree = train_CART(X, Y, 0, nb_max)\n",
    "        pseudoCode = str(self.tree)\n",
    "        if len(Y_name) > 0: \n",
    "            pseudoCode = pseudoCode.replace(\"Y\", Y_name)\n",
    "        for i in range(len(X_names)): \n",
    "            pseudoCode = pseudoCode.replace(\"X[\" + str(i) + \"]\", X_names[i])\n",
    "        self.pseudoCode = pseudoCode\n",
    "        self.X_names = X_names\n",
    "    \n",
    "    def predict(self, X): \n",
    "        predictions = []\n",
    "        for i in range(len(X)): \n",
    "            predictions.append(self.tree.predict(X[i, :]))\n",
    "        return predictions\n",
    "    \n",
    "    def graphviz(self): \n",
    "        nid, pseudoCode = self.tree.graphviz()\n",
    "        res = \"digraph Tree {\\n\"\n",
    "        res += \"node [shape=box] ;\"\n",
    "        for i in range(len(self.X_names)): \n",
    "            pseudoCode = pseudoCode.replace(\"X[\" + str(i) + \"]\", self.X_names[i])\n",
    "        res += pseudoCode\n",
    "        res += \"}\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_classifier = CART()\n",
    "cart_classifier.train(X_NumPlayOut, Y_NumPlayOut, X_names=[\"weather\", \"temperature\", \"humidity\", \"wind\"], Y_name=\"PlayOut\")\n",
    "print(cart_classifier.pseudoCode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question (Q4):**\n",
    "\n",
    "In the pseudo-code, we notice that the decision tree does not take into account the \"humidity\" and \"wind\" features.\n",
    " \n",
    "**Unique_Value(weather)** = {'sunny', 'cloudy', 'rainy'}\n",
    "\n",
    "**Unique_Value(temperature)** = {18, 20, 21, 22, 24, 27, 28, 30}\n",
    "\n",
    "**Unique_Value(humidity)** = {65, 70, 75, 78, 80, 85, 90, 95, 96} \n",
    "\n",
    "**Unique_Value(wind)** = {'no', 'yes'} \n",
    "\n",
    "- What can you say about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer (A4) :**\n",
    "[ Write your answer here ]\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C'est juste une visualisation du graphe\n",
    "# Si Ã§a ne marche pas, ce n'ai pas grave\n",
    "try:\n",
    "    from IPython.display import SVG\n",
    "    from graphviz import Source\n",
    "    from IPython.display import display\n",
    "    \n",
    "    graph = Source(cart_classifier.graphviz())\n",
    "    display(SVG(graph.pipe(format='svg')))\n",
    "\n",
    "except ImportError:\n",
    "    print(\"il faut installer graphviz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Benchmarking and analysis\n",
    "Here you are not going to code, but understand and analyse the results.\n",
    "\n",
    "#### 2.3.1 Decision Tree Generation\n",
    "Here, we will do a comparaison of ID3 vs CART based on how they generate the decision tree. \n",
    "\n",
    "**Reminder :**\n",
    "- ID3 :\n",
    "    - uses the entropy \n",
    "    - the number of node's children is according to the number of values\n",
    "- CART :\n",
    "    - uses the Gini index\n",
    "    - the number of node's leafs is binary\n",
    "    \n",
    "Here, we will use the \"**PlayOut.csv**\" dataset with nominal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id3_nom_classifier = ID3()\n",
    "id3_nom_classifier.train(X_playOut, Y_playOut, X_names=[\"weither\", \"temperature\", \"humidity\", \"wind\"], Y_name=\"PlayOut\")\n",
    "print(\"ID3 Pseudo-code :\\n\")\n",
    "print(id3_nom_classifier.pseudoCode)\n",
    "print(\"\\n--------------\\n\")\n",
    "cart_nom_classifier = CART()\n",
    "cart_nom_classifier.train(X_playOut, Y_playOut, X_names=[\"weither\", \"temperature\", \"humidity\", \"wind\"], Y_name=\"PlayOut\")\n",
    "print(\"CART Pseudo-code :\\n\")\n",
    "print(cart_nom_classifier.pseudoCode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze these results (Q5)**\n",
    "- Comparison between ID3 and CART according to their way of generating the tree.\n",
    "- Analyze the depth, the features used, the type of tree and maybe the impact on the prediction speed (worst case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer (A5):**\n",
    "[ Write your answer here ]\n",
    "\n",
    "ID3 :\n",
    "- ...\n",
    "- ...\n",
    "\n",
    "CART :\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a visualisation of the graph.\n",
    "# If it doesn't work, it isn't a big deal (you can move to the next cell).\n",
    "try:\n",
    "    from IPython.display import SVG\n",
    "    from graphviz import Source\n",
    "    from IPython.display import display\n",
    "    \n",
    "    print(\"Visualization of ID3 Decision Tree :\")\n",
    "    graph1 = Source(id3_nom_classifier.graphviz())\n",
    "    display(SVG(graph1.pipe(format='svg')))\n",
    "    \n",
    "    print(\"\\n-------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    print(\"Visualization of CART Decision Tree :\")\n",
    "    graph2 = Source(cart_nom_classifier.graphviz())\n",
    "    display(SVG(graph2.pipe(format='svg')))\n",
    "\n",
    "except ImportError:\n",
    "    print(\"graphviz must be installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Benchmarking based on Types of attributs  \n",
    "\n",
    "Here, we will compare between two CART classifiers applied on :\n",
    "\n",
    "- Nominal data (categorical); already trained\n",
    "- Mixed data (numerical and nominal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder : Be careful\n",
    "'''\n",
    "X_playOut, Y_playOut : contains only nominal data\n",
    "X_NumPlayOut, Y_NumPlayOut : contains numerical and nominal data\n",
    "'''\n",
    "\n",
    "print(\"CART with nominal features: :\")\n",
    "print(\"============================================\")\n",
    "print(cart_nom_classifier.pseudoCode)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "cart_num_classifier = CART()\n",
    "cart_num_classifier.train(X_NumPlayOut, Y_NumPlayOut, X_names=[\"weither\", \"temperature\", \"humidity\", \"wind\"], Y_name=\"PlayOut\")\n",
    "\n",
    "print(\"CART with some numerical features :\")\n",
    "print(\"================================================\")\n",
    "print(cart_num_classifier.pseudoCode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze these results (Q6)** :\n",
    "- Analyzes the impact of characteristic types.\n",
    " * Categorical data  \n",
    " * Heterogeneous data (categorical and numerical)\n",
    "- The analysis could be based on the generated pseudo-code (or tree) according to the previous criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer (A6) :**\n",
    "[ Write your answer here ]\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a visualisation of the graph.\n",
    "# If it doesn't work, it isn't a big deal (you can move to the next cell).\n",
    "\n",
    "try:\n",
    "    from IPython.display import SVG\n",
    "    from graphviz import Source\n",
    "    from IPython.display import display\n",
    "\n",
    "    print(\"Visualization of CART Decision Tree on only nominal features :\")\n",
    "    graph1 = Source(cart_nom_classifier.graphviz())\n",
    "    display(SVG(graph1.pipe(format='svg')))\n",
    "    \n",
    "    print(\"\\n-------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    print(\"Visualization of CART Decision Tree on heterogeneous data (features):\")\n",
    "    graph2 = Source(cart_num_classifier.graphviz())\n",
    "    display(SVG(graph2.pipe(format='svg')))\n",
    "\n",
    "except ImportError:\n",
    "    print(\"graphviz must be installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.2.3 Benchmarking Performance : Sklearn VS Our models \n",
    "\n",
    "- In this section, We compare our implementation of ID3 and CART model against a the high-level implementation of decision tree classifiers with the python \"Scikit-Learn\" library. \n",
    "\n",
    "- We will use [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) to classify flowers in three classes, using 4 features.\n",
    "\n",
    "We will train three models : \n",
    "- ID3 after discritization of the values\n",
    "- CART \n",
    "- Decision tree with scikit-learn library\n",
    "\n",
    "Description of the metrics : \n",
    "- the support is the number of samples\n",
    "- for each class, we provide the precision, the recall and the F1-score\n",
    "- micro avg : the metrics (precision, recall and F1-score) computed for the totality of the estimations (useful when the number of classes is unbalanced)\n",
    "- macro avg : the metrics computed for each class, then averaged \n",
    "- weighted avg: the metrics calculated for each class, multiplied by the equivalent support, summed, then divided by the total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "iris = pd.read_csv(\"data/iris.csv\")\n",
    "iris = iris.sample(frac=1)\n",
    "# Extraction des features \n",
    "X_iris = iris.iloc[:, :-1].values # PremiÃ¨res colonnes \n",
    "\n",
    "Y_iris = iris.iloc[:,-1].values # DerniÃ¨re colonne \n",
    "\n",
    "X_names = list(iris.columns)[:-1]\n",
    "Y_name = list(iris.columns)[-1]\n",
    "\n",
    "# Split data on two sets : Training Set (80%) - Test Set (20%)\n",
    "\n",
    "## create a mask for filtering dataset, by creating 80% of randomly true labls\n",
    "## e.g, for a dataset with 5 instances (examples, rows), \n",
    "## we could generate a boolean mask array like : [True, True, True, False, True] \n",
    "## where 80% of boolean values are \"True\"\n",
    "iris_msk = np.random.rand(len(X_iris)) < 0.8\n",
    "\n",
    "## Train set :  Rows with \"True\" labels in the mask array\n",
    "X_iris_train = X_iris[iris_msk]\n",
    "Y_iris_train = Y_iris[iris_msk]\n",
    "\n",
    "## Test set :  Rows with \"False\" labels in the mask array\n",
    "## to select these rows in pandas, we need to invert the values of the boolean mask array  \n",
    "## for the previous example we will get the following mask array : [False, False, False, True, False] \n",
    "## ~ : it's a logical not operation \n",
    "\n",
    "X_iris_test = X_iris[~iris_msk]\n",
    "Y_iris_test = Y_iris[~iris_msk]\n",
    "\n",
    "# print first 20 rows\n",
    "iris.head(20)\n",
    "#X_names, Y_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traget classes : we have 3 classes\n",
    "print(np.unique(Y_iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building Our Models :\n",
    "\n",
    "##### 1\\ ID3 after discritization\n",
    "\n",
    "- STEP 01 : Deatures Discritization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import frome sklearn library the KBinsDiscretizer function : \n",
    "# to transforme continuous data into intervals (discritization) .\n",
    "# Check the documentation : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html\n",
    "from sklearn.preprocessing import KBinsDiscretizer \n",
    "\n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "# Fit the data\n",
    "est.fit(X_iris)\n",
    "# Transform the date\n",
    "X_iris_disc = est.transform(X_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 rows\n",
    "X_iris_disc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- STEP 02 : Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get \"Training\" data out the uniformally discretized data\n",
    "X_iris_disc_train = X_iris_disc[iris_msk]\n",
    "\n",
    "# Train the model Like Cristiano Ronaldo\n",
    "id3_iris = ID3()\n",
    "id3_iris.train(X_iris_disc_train, Y_iris_train, X_names=X_names, Y_name=Y_name, pruning=True)\n",
    "\n",
    "# Evaluation : predict the the unseen test attributs  \n",
    "# Get \"Testing\" data out the uniformally discretized data\n",
    "X_iris_disc_test = X_iris_disc[~iris_msk]\n",
    "id3_iris_res = id3_iris.predict(X_iris_disc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2\\ CART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CART classifier\n",
    "cart_iris = CART()\n",
    "cart_iris.train(X_iris_train, Y_iris_train, X_names=X_names, Y_name=Y_name)\n",
    "cart_iris_res = cart_iris.predict(X_iris_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3\\ Build and Train a Decision Tree (classifier) with scikit-learn library\n",
    "- **Scikit-learn (Sklearn)** is a python library for machine learning. It provides a selection of tools for machine learning and statistical modeling : including classification, regression and clustering ... via a Python interface. This library, which is largely written in Python, is built upon NumPy, SciPy and Matplotlib.\n",
    "- To builf decision trees with this high level library we call the **DecisionTreeClassifier** class. \n",
    "- **DecisionTreeClassifier :** is a class capable of performing multi-class classification on a dataset.\n",
    "- Check Also : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "\n",
    "\n",
    "**NB:** *In the Scikit-learn implementation of decision trees, the features are randomly swapped at each division. This makes the tree non-deterministic. To stop this, we use the property random_state=0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "sklearn_cart_iris = DecisionTreeClassifier(random_state=0)\n",
    "sklearn_cart_iris.fit(X_iris_train, Y_iris_train)\n",
    "sklearn_cart_iris_res = sklearn_cart_iris.predict(X_iris_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmarking :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classification evaluation report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"ID3\")\n",
    "print(classification_report(Y_iris_test, id3_iris_res))\n",
    "\n",
    "\n",
    "print(\"CART\")\n",
    "print(classification_report(Y_iris_test, cart_iris_res))\n",
    "\n",
    "print(\"Scikit-learn\")\n",
    "print(classification_report(Y_iris_test, sklearn_cart_iris_res))\n",
    "\n",
    "# Ignore the warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze these results (Q7):**\n",
    "\n",
    "- Test the performance of three models\n",
    "- The comparison must be based on the metrics : precision, recall and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer (A7):**\n",
    "[ Write your answer here ]\n",
    "\n",
    "ID3 :\n",
    "- ...\n",
    "- ...\n",
    "\n",
    "CART :\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "1. [Quinlan, J. R. (1986). Induction of decision trees. Machine learning, 1(1), 81-106.](https://link.springer.com/content/pdf/10.1007/BF00116251.pdf)\n",
    "\n",
    "2. [Quinlan, J. R. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993.](https://books.google.fr/books?hl=fr&lr=&id=b3ujBQAAQBAJ&oi=fnd&pg=PP1&dq=C4.+5:+Programs+for+machine+learning&ots=sR7pRTJuC2&sig=hmtbed3-eUljdUvf6eoM2vfrbHQ&redir_esc=y#v=onepage&q=C4.%205%3A%20Programs%20for%20machine%20learning&f=false)\n",
    "\n",
    "4. [Breiman, L., Freidman, J.H., Olshen, R.A., & Stone, C.J. (1984). CART: Classification and Regression Trees.](http://www.math.ku.dk/~richard/courses/statlearn2009/lecture10.pdf).\n",
    "5. [Shannon, C. E. (1948). A mathematical theory of communication. The Bell system technical journal, 27(3), 379-423.](https://pure.mpg.de/rest/items/item_2383164/component/file_2383163/content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
