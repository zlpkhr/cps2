{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a656003",
   "metadata": {},
   "source": [
    "# LAB 05 : Implementation of Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf75f4",
   "metadata": {},
   "source": [
    "**Full Name** : Azamat ZULPYKHAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffd01d",
   "metadata": {},
   "source": [
    "## I- Implementation of a Neural Network and its Algorithms\n",
    "- We will use this example to check the realization\n",
    "- In this computational graph,  we adopt the following notations :\n",
    "    - $a_i^{(L)}$ : The acctivation score at i'th *node* of the L'th *layer* (Where $a_1^{(1)}$ of the input layer is the value of the 1'st feature of an arbitrary example from the set X of all instances ).\n",
    "    - $Z_i^{(L)} = {W^{(L)}}^{T} * a^{(L-1)}+ b^{(L)}$ : The linear sum at the **L'th layer**.\n",
    "    - Where $b^{(L)}$ : the bias (learned or initialized) at the **L'th layer**.  \n",
    "    - $f_i^{(L)}$ : is the activation function applied at the **L'th layer**.\n",
    "    - Also , $a_i^{(L)}= f_i^{(L)}(Z_i^{(L)})$ \n",
    "    - $Y_i$ : the label of the i'th training example.\n",
    "    - $J$ : Loss function \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24766d",
   "metadata": {},
   "source": [
    "![exemple](RNPA-exp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6b164da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import colors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f611d0",
   "metadata": {},
   "source": [
    "Here we define an API (a kind of interface) for the activation functions and the cost functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd73a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API\n",
    "class Activation(object): \n",
    "    # Calculate the activation based on Z (the linear sum)\n",
    "    def activate(self, Z):\n",
    "        pass\n",
    "    # Calculate the derivative based on Z and the activation A\n",
    "    # The activation value in the last layer (prediction node) \n",
    "    # is equal the prediction or hypothesis value, thus H\n",
    "    def derive(self, Z, H):\n",
    "        pass\n",
    "\n",
    "# API\n",
    "class Cost(object): \n",
    "    # Calculate the activation based on Z (the linear sum)\n",
    "    def calculate(self, H, Y):\n",
    "        pass\n",
    "    # Calculate the derivative based on Z and the activation A (H: hypothesis or predicted value)\n",
    "    def derive(self, H, Y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04104b6",
   "metadata": {},
   "source": [
    "### I.1 The activation functions\n",
    "In the following jupyter cells, we will implement the following two activation functions ($\\sigma_1$ and $\\sigma_2$): \n",
    "- The **Logistic activation function** \n",
    "    - It is calculated as follows : \n",
    "$$A = \\sigma_1(Z) = \\frac{1}{1+e^{-Z}}$$\n",
    "    - It's partial derivative is given by :\n",
    "$$\\frac{\\partial \\sigma_1(Z)}{\\partial \\theta} = \\sigma_1(Z) (1-\\sigma_1(Z))$$\n",
    "-  The **Hyperbolic tangent (tanh) activation function** :\n",
    "    - It is calculated as follows : \n",
    "$$A = \\sigma_2(Z) = \\frac{e^Z - e^{-Z}}{e^Z + e^{-Z}}$$\n",
    "    - It's partial derivative is given by : \n",
    "$$\\frac{\\partial \\sigma_2(Z)}{\\partial \\theta} = 1-\\sigma_2(Z)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f65fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "# TODO perform the derivative of the logistic activation function\n",
    "\n",
    "def sigmaf(Z):\n",
    "    ### CODE 01 ###\n",
    "    \n",
    "    ### BEGIN : Write your code here\n",
    "    \n",
    "    numerator = 1\n",
    "    \n",
    "    denominator =1 + np.exp(-Z)\n",
    "    \n",
    "    ### END\n",
    "    return numerator/ denominator\n",
    "\n",
    "def d_sigmaf(Z, A): \n",
    "    ### CODE 02 ###\n",
    "    \n",
    "    ### BEGIN : Write your code here\n",
    "    \n",
    "    activation = A\n",
    "    \n",
    "    result = activation * (1-activation)\n",
    "    \n",
    "    ### END\n",
    "    \n",
    "    return result\n",
    "\n",
    "# TODO perform the derivative of the \"tanh\" activation function\n",
    "\n",
    "def tanhf(Z):\n",
    "    ### CODE 03 ###\n",
    "    \n",
    "    ### BEGIN : Write your code here\n",
    "    \n",
    "    numerator = np.exp(Z)-np.exp(-Z)\n",
    "    \n",
    "    denominator = np.exp(Z)+np.exp(-Z)\n",
    "    \n",
    "    ### END\n",
    "    \n",
    "    return numerator/ denominator\n",
    "\n",
    "def d_tnahf(Z, A): \n",
    "    ### CODE 04 ###\n",
    "    \n",
    "    ### BEGIN : Write your code here\n",
    "    \n",
    "    result = 1-A**2\n",
    "    \n",
    "    ### END\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create the logitic and tanh classes following the API we defined \n",
    "class Logistic(Activation):\n",
    "    def activate(self, Z):\n",
    "        return sigmaf(Z)\n",
    "    def derive(self, Z, H):\n",
    "        return d_sigmaf(Z, H)\n",
    "\n",
    "class Tanh(Activation):\n",
    "    def activate(self, Z):\n",
    "        return tanhf(Z)\n",
    "    def derive(self, Z, H):\n",
    "        return d_tnahf(Z, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4c9b78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the z 1st value in the 4th layer : [1.666 1.68 ]\n",
      "\n",
      "The logistic activation function :\n",
      "\n",
      "- Logistic(z4_1) =  [0.84104179 0.84290453]\n",
      "- d_Logistic(z4_1,a4_1) =  [0.1336905  0.13241648]\n",
      "\n",
      "The tanh activation function :\n",
      "\n",
      "- tanh(z) =  [0.93102086 0.93286155]\n",
      "- d_tanh(z4_1,a4_1) =  [0.13320015 0.12976932]\n"
     ]
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TESTING (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Results :\n",
    "#\n",
    "# the z 1st value in the 4th layer : [1.666 1.68 ]\n",
    "#\n",
    "# The logistic activation function :\n",
    "#\n",
    "# - Logistic(z4_1) =  [0.84104179 0.84290453]\n",
    "# - d_Logistic(z4_1,a4_1) =  [0.1336905  0.13241648]\n",
    "#\n",
    "# The tanh activation function :\n",
    "#\n",
    "# - tanh(z) =  [0.93102086 0.93286155]\n",
    "# - d_tanh(z4_1,a4_1) =  [0.13320015 0.12976932]\n",
    "#\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "logitic = Logistic()\n",
    "tanh = Tanh()\n",
    "# Let's take the Z value of the 1st node of the 4th layer z4_1: [1.666, 1.68]\n",
    "z4_1 = np.array([1.666, 1.68])\n",
    "\n",
    "# Calculate the activation function (Logistic) and its derivation on z4_1 \n",
    "logistic_a4_1 = logitic.activate(z4_1)\n",
    "logisticDerivative_a4_1 = logitic.derive(z4_1, logistic_a4_1)\n",
    "\n",
    "# Calculate the activation function (tanh) and its derivation on the following array \n",
    "tanh_a4_1 = tanh.activate(z4_1)\n",
    "tanhDerivative_a4_1 = tanh.derive(z4_1, tanh_a4_1)\n",
    "\n",
    "print(\"the z 1st value in the 4th layer :\", z4_1)\n",
    "print(\"\\nThe logistic activation function :\\n\")\n",
    "print('- Logistic(z4_1) = ',logistic_a4_1) \n",
    "print('- d_Logistic(z4_1,a4_1) = ',logisticDerivative_a4_1)\n",
    "print(\"\\nThe tanh activation function :\\n\")\n",
    "print('- tanh(z) = ',tanh_a4_1) \n",
    "print('- d_tanh(z4_1,a4_1) = ',tanhDerivative_a4_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b5cc2",
   "metadata": {},
   "source": [
    "**NB :** In the example above, you can see that the results are similar to those obtained with the logistic activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b83f90",
   "metadata": {},
   "source": [
    "### I.2. The cost functions\n",
    "\n",
    "- **Binary Cross-Entropy (BCE)**: \n",
    "\n",
    "    - The BCE function is calculated as :\n",
    "$$BCE (H_i, Y_i)= - ( Y_i \\log(H_i) + (1-Y_i) \\log(1-H_i))$$\n",
    "\n",
    "    - Its derivative is calculated as :\n",
    "$$\\frac{\\partial BCE (H_i, Y_i)}{\\partial \\theta} = \\frac{H_i-Y}{H_i - H_i^2}$$\n",
    "\n",
    "Where the variables (for a given instance $X_i$):\n",
    "- $H_i$ : is the hypothesis, or the prediction that the neural network made on $X_i$.\n",
    "- $Y_i$ : is the true label of $X_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c159781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO code the derivative of the BCE error function\n",
    "\n",
    "def bce_function(H, Y):\n",
    "    ### CODE 05 : BEGIN ###\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    result = -(Y*np.log(H)+(1-Y)*np.log(1-H))\n",
    "    \n",
    "    #### CODE 05 : END ###\n",
    "    \n",
    "    return result \n",
    "\n",
    "def d_bce_function(H, Y):\n",
    "    ### CODE 06 : BEGIN ###\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    numerator = H-Y\n",
    "    \n",
    "    denominator = H-np.square(H)\n",
    "    \n",
    "    result = numerator/denominator\n",
    "    \n",
    "    #### CODE 06 : END ###\n",
    "    \n",
    "    return result\n",
    "\n",
    "class BCE(Cost):\n",
    "    def calculate(self, H, Y):\n",
    "        return bce_function(H, Y)\n",
    "    def derive(self, H, Y):\n",
    "        return d_bce_function(H, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d35047f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_BCE(H,Y) =  [1.83258146 0.17078832]\n",
      "Derivative of Loss_BCE(H,Y) =  [ 6.25       -1.18623962]\n"
     ]
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TESTING (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Results :\n",
    "#\n",
    "# Loss_BCE(H,Y) =  [1.83258146 0.17078832]\n",
    "# Derivative of Loss_BCE(H,Y) =  [ 6.25       -1.18623962]\n",
    "#\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "bce = BCE()\n",
    "\n",
    "# The true labels of the two instances\n",
    "Y = np.array([0., 1.])\n",
    "# the predictions for the two instances , H=[H_0, H_1] for X_0 and X_1\n",
    "H = np.array([0.840 , 0.843]) \n",
    "\n",
    "# Calculate the loss in predictions\n",
    "J = bce.calculate(H, Y)\n",
    "\n",
    "# The derivative of the loss function is useful for backpropagation. \n",
    "# It helps the optimization algorithm to rectify the weights \"W^(l)\".\n",
    "DJ = bce.derive(H, Y)\n",
    "\n",
    "print(\"Loss_BCE(H,Y) = \", J)\n",
    "print(\"Derivative of Loss_BCE(H,Y) = \", DJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30132a60",
   "metadata": {},
   "source": [
    "### I.3. Neuron\n",
    "\n",
    "The function that updates the parameters (weights and biases) takes as input : \n",
    "- $W[Lp]$ a list of weights; a vector of size $Lp$ (the number of neurons in the previous layer).\n",
    "- $b$ the bias.\n",
    "- $Z[M]$ lthe linear combination of the current neuron; a vector of size $M$ (the number of samples).\n",
    "- $A[M]$ the activation of the current neuron; a vector of size $M$.\n",
    "- $A\\_past[M, Lp]$ the activations of the neurons of the previous layer; a size matrix is $(M * Lp)$.\n",
    "- $Delta\\_next[M, Ln]$ the delta calculated in the next layer; a matrix of size $M * Ln$ ($Ln$ : the number of neurons in the next layer).\n",
    "- $W\\_next[Ln]$ the weights to the next layer; a vector of size $Ln$.\n",
    "- $act$ :  it is an object of type \"Activation\"; it provides two methods: \"act.activate\" and \"act.derive\".\n",
    "- $alpha$  : the learning rate (training step).\n",
    "\n",
    "---------------\n",
    "**Parameters updating equations :**\n",
    "$$ W_i^{(L)} = W_i^{(L)} - \\alpha * \\frac{\\partial J}{\\partial w^{(L)}} $$\n",
    "$$ b_i^{(L)} = b_i^{(L)} - \\alpha * \\frac{\\partial J}{\\partial b^{(L)}} $$\n",
    "\n",
    "**The backpropagation equations (for single example):**\n",
    "\n",
    "\n",
    "- $\\delta^{(L)} = \\frac{\\partial f^{(L)}}{\\partial z^{(L)}} w^{(L+1)} \\delta^{(L+1)} = f'(z^{(L)})  w^{(L+1)} \\delta^{(L+1)}$ : denote this by ```delta``` in code\n",
    "\n",
    "- $\\frac{\\partial J}{\\partial w^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} \\delta^{(L)}= a^{(L-1)} \\delta^{(L)}$ : denote this by ```dw``` in code\n",
    "\n",
    "- $\\frac{\\partial J}{\\partial b^{(L)}} = \\delta^{(L)}$ : denote this by ```db``` in code\n",
    "\n",
    "where :\n",
    "- $\\delta_i^{(L)}$ : is the error of i'th node in the layer L.  \n",
    "\n",
    "NB : In the code, when we update the parameters we take the average of the gradient on all the samples. \n",
    "\n",
    "Check the following links (videos) for more insight on how we got the following famous 3 equations of the backpropagation : \n",
    "- [Backpropagation calculus | Chapter 4, Deep learning](https://www.youtube.com/watch?v=tIeHLnjs5U8) (**recommended**).\n",
    "- [Lecture 9.2 — Neural Networks Learning | Backpropagation Algorithm — [ Machine Learning | Andrew Ng]](https://youtu.be/x_Eamf8MHwU?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN)\n",
    "- [Lecture 9.3 — Neural Networks Learning | Backpropagation Intuition — [ Machine Learning | Andrew Ng]](https://youtu.be/mOmkv5SI9hU?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ebd2e",
   "metadata": {},
   "source": [
    "Hints :\n",
    "- The deltas are calculated with respect to the different outputs. \n",
    "- when we update the parameters we take the average of the gradient on all the samples. \n",
    "- The reason why we don't calculate the average only in the output layer is because we do the back-propagation for each sample. \n",
    "\n",
    "- I put moy() to specify that we have to take the average of the samples when we update the parameters. But, when we broadcast the gradients backwards, we treat each sample separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1819ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement the function that updates the weights of a neuron\n",
    "\n",
    "def neuron_update(W, b, Z, A, A_past, Delta_next, W_next, act, alpha=1.):\n",
    "    ### CODE 07 : BEGIN ###\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    # get the number of sample \n",
    "    m = Z.shape[0]\n",
    "    \n",
    "    # Use np.multiply() and np.dot() function (check the official documentation)\n",
    "    # np.multiply() : for element-wise multiplication.\n",
    "    # np.dot() : is for Dot product of two arrays. \n",
    "    delta = act.derive(Z, A_past) * np.dot(W_next,Delta_next)\n",
    "    \n",
    "    # take into account all the samples \n",
    "    # use np.sum() or np.dot() to calculate the avreage of all dw and db\n",
    "    dw = (1/m) * np.dot(A_past, delta)\n",
    "    db = (1/m) * np.sum(delta)\n",
    "    \n",
    "    # Updating the weights\n",
    "    W_new = W - alpha * dw\n",
    "    b_new = b - alpha * db\n",
    "    \n",
    "    ### CODE 07 : END ###\n",
    "    \n",
    "    return W_new, b_new, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "011d4def",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "output array is not acceptable (must have the right datatype, number of dimensions, and be a C-Array)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Set Logistic function as the activation function\u001b[39;00m\n\u001b[1;32m     29\u001b[0m act \u001b[38;5;241m=\u001b[39m Logistic() \n\u001b[0;32m---> 31\u001b[0m W_new, b_new, Delta_new \u001b[38;5;241m=\u001b[39m \u001b[43mneuron_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_past_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDelta_next_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_next_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated Weight :\u001b[39m\u001b[38;5;124m\"\u001b[39m,W_new)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated bias :\u001b[39m\u001b[38;5;124m\"\u001b[39m,b_new) \n",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m, in \u001b[0;36mneuron_update\u001b[0;34m(W, b, Z, A, A_past, Delta_next, W_next, act, alpha)\u001b[0m\n\u001b[1;32m      9\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(Z)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Use np.multiply() and np.dot() function (check the official documentation)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# np.multiply() : for element-wise multiplication.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# np.dot() : is for Dot product of two arrays. \u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mderive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_past\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43mDelta_next\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# take into account all the samples \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# use np.sum() or np.dot() to calculate the avreage of all dw and db\u001b[39;00m\n\u001b[1;32m     18\u001b[0m dw \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(A_past, delta)\n",
      "\u001b[0;31mValueError\u001b[0m: output array is not acceptable (must have the right datatype, number of dimensions, and be a C-Array)"
     ]
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TESTING (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Results :\n",
    "# \n",
    "# Updated Weight : [0.49375218 0.2046736 ]\n",
    "# Updated bias : -0.30324311474187016\n",
    "# Delta error propagation : [ 0.00696306 -0.00047683]\n",
    "# \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "W_t = np.array([0.5, 0.2])\n",
    "b_t = -0.3\n",
    "Z_t = np.array([0.5, 2.2])\n",
    "\n",
    "# M (the current activation)\n",
    "A_t = np.array([0.62245933, 0.90024951])\n",
    "\n",
    "# M * L (the activations of the previous layer)\n",
    "A_past_t = np.array([[2., -1.], \n",
    "                     [3., 5.]])\n",
    "\n",
    "# L\n",
    "Delta_next_t = np.array([[ 0.14523862, -0.02613822], \n",
    "                         [ 0.1394202, -0.02531591]]).T\n",
    "W_next_t = np.array([0.3, -0.1])\n",
    "\n",
    "# Set Logistic function as the activation function\n",
    "act = Logistic() \n",
    "\n",
    "W_new, b_new, Delta_new = neuron_update(W_t, b_t, Z_t, A_t, A_past_t, Delta_next_t, W_next_t, act, alpha=1.)\n",
    "\n",
    "print(\"Updated Weight :\",W_new)\n",
    "print(\"Updated bias :\",b_new) \n",
    "print(\"Delta error propagation :\",Delta_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef48691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "    def __init__(self, input_shape, activation=Logistic()):\n",
    "        # Initialize the parameters to zero \n",
    "        self.w = np.array([0.] * input_shape)\n",
    "        self.b = 0.\n",
    "        # Set the activation function\n",
    "        self.act = activation\n",
    "        \n",
    "    def randomize(self):\n",
    "        self.w = np.random.rand(len(self.w))\n",
    "        self.b = np.random.rand(1)[0]\n",
    "        \n",
    "    def __aggregate(self, X):\n",
    "        # Calculate Z, the linear sum\n",
    "        return np.dot(X, self.w) + self.b\n",
    "    \n",
    "    def activate(self, X):\n",
    "        self.a_past = X\n",
    "        self.z = self.__aggregate(X)\n",
    "        self.a = self.act.activate(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    def update(self, delta_next, w_next, alpha=1.):\n",
    "        w_previous = self.w.copy()\n",
    "        self.w, self.b, delta = neuron_update(self.w, self.b, self.z, self.a, self.a_past, \n",
    "                                              delta_next, w_next, self.act, alpha=alpha)\n",
    "        return delta, w_previous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TESTING (Run the cell and compare your results)\n",
    "#=====================================================================\n",
    "# Results :\n",
    "#\n",
    "# z2_1 = [0.5 2.2]\n",
    "# a2_1 = [0.62245933 0.90024951]\n",
    "# derive(a2_1) = [0.23500371 0.08980033]\n",
    "# previous b = -0.3\n",
    "# previous w = [0.5 0.2]\n",
    "# delta2 = [ 0.00696306 -0.00047683]\n",
    "# updated b = -0.30324311473938026\n",
    "# updated w = [0.49375218 0.2046736 ]\n",
    "#\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Creation of a neuron with two inputs\n",
    "n = Neuron(2)\n",
    "# ---------------------\n",
    "# We don't have to assign the weights directly \n",
    "# Here, it's just to have the same weights of the output neuron in the example\n",
    "\n",
    "# We will reproduce the parameters of the neuron 1 hidden layer 1 (layer 2)\n",
    "n.b = -0.3\n",
    "n.w = np.array([0.5, 0.2])\n",
    "# ---------------------\n",
    "\n",
    "# M x Lp (here it is X : input layer)\n",
    "A1 = np.array([[2., -1.], [3., 5.]])\n",
    "# M x Ln (Delta of the next layer)\n",
    "Delta3 = np.array([[ 0.14523862, -0.02613822], [ 0.1394202, -0.02531591]]).T\n",
    "W3_1 = np.array([0.3, -0.1])\n",
    "\n",
    "\n",
    "A2_1 = n.activate(A1)\n",
    "print(\"z2_1 = \" + str(n.z))\n",
    "print(\"a2_1 = \" + str(A2_1))\n",
    "\n",
    "# the derivative of the logistic function does not need z, so we pass 0\n",
    "print(\"derive(a2_1) = \" + str(n.act.derive(0,A2_1)))\n",
    "print(\"previous b = \" + str(n.b))\n",
    "\n",
    "Delta2, W2_previous = n.update(Delta3, W3_1) \n",
    "\n",
    "print(\"previous w = \" + str(W2_previous))\n",
    "print(\"delta2 = \" + str(Delta2))\n",
    "print(\"updated b = \" + str(n.b))\n",
    "print(\"updated w = \" + str(n.w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a7c31",
   "metadata": {},
   "source": [
    "### I.4. Neural Network : The Layer\n",
    "\n",
    "**Nothing to code here.**\n",
    "\n",
    "- Here we code a class that defines a layer by indicating :\n",
    "    - The number of neurons (size)\n",
    "    - The number of these inputs \n",
    "    - The activation function of these neurons.\n",
    "    \n",
    "    \n",
    "- This class includes 3 methods : \n",
    "    1. initializer : to initialize the parameters (weights and biases) of the neurons in a random way.\n",
    "    2. forwardPass :to apply the forward propagation.\n",
    "    3. backPass : to apply the backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ae69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self, units, input_shape, activation=Logistic()):\n",
    "        self.neurons = [Neuron(input_shape, activation=activation) for i in range(units)]\n",
    "        \n",
    "    def randomize(self):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.randomiser()\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activations = []\n",
    "        for neuron in self.neurons:\n",
    "            activations.append(neuron.activate(X))\n",
    "        return np.array(activations).T\n",
    "    \n",
    "    def back_propagation(self, delta_next, W_next, alpha=1.):\n",
    "        W_previous = []\n",
    "        Deltas = []\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            delta, w_previous = neuron.update(delta_next, W_next[i], alpha=alpha)\n",
    "            W_previous.append(w_previous)\n",
    "            Deltas.append(delta)\n",
    "        return np.array(Deltas).T, np.array(W_previous).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# activations : [[0.62245933 0.66818777]\n",
    "#  [0.90024951 0.96770454]]\n",
    "# deltas : [[ 0.00696306  0.00682726]\n",
    "#  [-0.00047683 -0.00017109]]\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Layer 2\n",
    "c2 = Layer(2, 2)\n",
    "\n",
    "# We don't have to assign the weights directly \n",
    "# Here, it's just to have the same weights of the output neuron in the course example\n",
    "c2.neurons[0].b = -0.3\n",
    "c2.neurons[0].w = np.array([0.5, 0.2])\n",
    "c2.neurons[1].b = 0.5\n",
    "c2.neurons[1].w = np.array([0.3, 0.4])\n",
    "\n",
    "a2 = np.array([[2., -1.], [3., 5.]])\n",
    "\n",
    "# L\n",
    "delta3 = np.array([[ 0.14523862, -0.02613822], [ 0.1394202, -0.02531591]]).T\n",
    "w3 = np.array([[0.3, -0.1],[0.5, -0.3]])\n",
    "\n",
    "# M x Lp (here it is X : input layer)\n",
    "a1 = np.array([[2., -1.], [3., 5.]])\n",
    "a2 = c2.forward_propagation(a1)\n",
    "print(\"activations : \" + str(a2))\n",
    "\n",
    "Deltas2, W_anciens2 = c2.back_propagation(delta3, w3)\n",
    "\n",
    "print(\"deltas : \" + str(Deltas2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd65337",
   "metadata": {},
   "source": [
    "### I.5. The Neural Network : Build The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7447314",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, output_shape, cost=bce, alpha=1.):\n",
    "        # The size of the last layer\n",
    "        self.current_shape = output_shape \n",
    "        # object of type Cost to calculate the cost and its derivative\n",
    "        self.cost = cost \n",
    "        self.alpha = alpha\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, units, activation=Logistic()):\n",
    "        new_layer = Layer(units, self.current_shape, activation=activation)\n",
    "        self.layers.append(new_layer)\n",
    "        self.current_shape = units\n",
    "        \n",
    "    def randomize(self):\n",
    "        for layer in self.layers:\n",
    "            layer.randomize()\n",
    "    \n",
    "    def predict(self, X): \n",
    "        Y = X\n",
    "        if self.norm:\n",
    "            Y = np.where(self.std==0, X, (X - self.mean)/self.std)\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            Y = layer.forward_propagation(Y)\n",
    "        if Y.ndim == 2 and Y.shape[1] == 1:\n",
    "            Y = Y.flatten()\n",
    "        return np.where(Y < 0.5, 0, 1)\n",
    "    \n",
    "    \n",
    "    def _do_iteration(self, X, Y):\n",
    "        # 1- Do forward propagation\n",
    "        a = X\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward_propagation(a)\n",
    "            \n",
    "        # 2- calculation of the cost and its derivative \n",
    "        YY = np.array(Y)\n",
    "        if YY.ndim < 2 : \n",
    "            YY = YY[:, np.newaxis]\n",
    "        J = np.mean(self.cost.calculate(a, YY))\n",
    "        J_prime = self.cost.derive(a, YY)\n",
    "        \n",
    "        # 3- Do back propagation \n",
    "        w_past = np.array([[1.] * self.current_shape])\n",
    "        delta_past = J_prime\n",
    "        # we start from the last layer to the first one\n",
    "        for layer in reversed(self.layers): \n",
    "            delta_past, w_past = layer.back_propagation(delta_past, w_past)\n",
    "        return J\n",
    "    \n",
    "    def train(self, X, Y, nbr_it=100, norm=False):\n",
    "        costs = []\n",
    "        X_norm = X\n",
    "        self.norm = norm\n",
    "        if norm:\n",
    "            self.mean = np.mean(X, axis=0)\n",
    "            self.std = np.std(X, axis=0)\n",
    "            X_norm = np.where(self.std==0, X, (X - self.mean)/self.std)\n",
    "\n",
    "        for i in range(nbr_it): \n",
    "            J = self._do_iteration(X_norm, Y)\n",
    "            costs.append(J)\n",
    "        return costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fa3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# The cost = 1.0020916974430965\n",
    "# w4_1 = [0.51494626 0.56592079]\n",
    "# w3_1 = [0.2665629 0.4641237]\n",
    "# w3_2 = [-0.13199638 -0.33433028]\n",
    "# w2_1 = [0.49375219 0.2046736 ]\n",
    "# w2_2 = [0.29342937 0.40384135]\n",
    "# The prediction : [0 1]\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X = np.array([[2., -1.], [3., 5.]])\n",
    "Y = np.array([0., 1.])\n",
    "\n",
    "nn = NeuralNetwork (2) # init a neural network with two input features\n",
    "nn.add_layer(2)        # add a layer with 2 neurons (hidden)\n",
    "nn.add_layer(2)        # add a layer with 2 neurons (hidden)\n",
    "nn.add_layer(1)        # add a layer with 1 neuron (output)\n",
    "\n",
    "# We don't have to assign the weights directly \n",
    "# Here, it's just to have the same weights of the output neuron in the above example\n",
    "\n",
    "nn.layers[0].neurons[0].b = -0.3\n",
    "nn.layers[0].neurons[0].w = np.array([0.5, 0.2])\n",
    "nn.layers[0].neurons[1].b = 0.5\n",
    "nn.layers[0].neurons[1].w = np.array([0.3, 0.4])\n",
    "\n",
    "nn.layers[1].neurons[0].b = -0.3\n",
    "nn.layers[1].neurons[0].w = np.array([0.3, 0.5])\n",
    "nn.layers[1].neurons[1].b = -0.2\n",
    "nn.layers[1].neurons[1].w = np.array([-0.1, -0.3])\n",
    "\n",
    "nn.layers[2].neurons[0].b = 1.\n",
    "nn.layers[2].neurons[0].w = np.array([0.7, 0.7])\n",
    "\n",
    "J = nn._do_iteration(X, Y)\n",
    "\n",
    "print(\"The cost= \" + str(J))\n",
    "print(\"w4_1 = \" + str(nn.layers[2].neurons[0].w))\n",
    "print(\"w3_1 = \" + str(nn.layers[1].neurons[0].w))\n",
    "print(\"w3_2 = \" + str(nn.layers[1].neurons[1].w))\n",
    "print(\"w2_1 = \" + str(nn.layers[0].neurons[0].w))\n",
    "print(\"w2_2 = \" + str(nn.layers[0].neurons[1].w))\n",
    "\n",
    "# Train our model\n",
    "nn.train(X, Y, nbr_it=200)\n",
    "print(\"The prediction : \" + str(nn.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac110a",
   "metadata": {},
   "source": [
    "## II. Application et analyse (Regression task)\n",
    "- We will use the dataset [Diabetics prediction using logistic regression](https://www.kaggle.com/kandij/diabetes-dataset). \n",
    "- The following configuration is set up like this on purpose\n",
    "- It is to test the case where regression is not favored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06fc915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diabetes2\n",
    "diabetes = pd.read_csv(\"datasets/diabetes2.csv\") \n",
    "X_diabetes = diabetes.iloc[:, :-1].values  \n",
    "Y_diabetes = diabetes.iloc[:, -1].values\n",
    "\n",
    "NBR_TEST = 240\n",
    "\n",
    "# Assuming the first 30% of lines are for testing and the rest for training\n",
    "X_test = X_diabetes[-NBR_TEST:, :] # 30% or more\n",
    "Y_test = Y_diabetes[-NBR_TEST:].reshape([-1, 1])\n",
    "\n",
    "X_train = X_diabetes[:-NBR_TEST, :] \n",
    "Y_train = Y_diabetes[:-NBR_TEST].reshape([-1, 1])\n",
    "\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651fdaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_trains = scaler.fit_transform(X_train)\n",
    "X_tests = scaler.transform(X_test)\n",
    "\n",
    "X_trains[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da979afe",
   "metadata": {},
   "source": [
    "#### II.1. Parameter initialization and model's complexity\n",
    "\n",
    "- Here, we want to test the usefulness of the initialization of the parameters (theta) and the complexity of the model.\n",
    "- To do this, we trained 5 models and obtained the training and validation error. \n",
    "\n",
    "\n",
    "The tested models are :\n",
    "- **Log0** : A single neuron (logistic regression) with initialization of parameters with the 0 value. \n",
    "- **LogR** : Un seule neurone (régression logistique) avec initialisation aléatoire\n",
    "- **RN0** : Un réseau de neurone 4(relu)X2(relu)X1(sigmoid) avec initialisation 0\n",
    "- **RN1** : Un réseau de neurone 4(relu)X2(relu)X1(sigmoid) avec initialisation 1\n",
    "- **RNR** : Un réseau de neurone 4(relu)X2(relu)X1(sigmoid) avec initialisation aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "alpha=0.01\n",
    "NBR_IT = 200\n",
    "\n",
    "M, N = X_train.shape\n",
    "\n",
    "reg0 = Sequential()\n",
    "reg0.add(Dense(1, activation=\"sigmoid\", kernel_initializer='zero', bias_initializer='zeros'))\n",
    "\n",
    "reg0.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=alpha))\n",
    "\n",
    "print(\"Training : regression theta=0 ...\")\n",
    "results = reg0.fit(X_trains, Y_train, epochs=NBR_IT, validation_data=(X_tests, Y_test), verbose=0)\n",
    "\n",
    "regr = Sequential()\n",
    "regr.add(Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "\n",
    "regr.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=alpha))\n",
    "\n",
    "print(\"Training : regression theta=random ...\")\n",
    "resultsr = regr.fit(X_trains, Y_train, validation_data=(X_tests, Y_test), epochs=NBR_IT, verbose=0)\n",
    "\n",
    "\n",
    "reg_m0 = Sequential()\n",
    "reg_m0.add(Dense(4, activation=\"relu\", kernel_initializer='zero', bias_initializer='zeros'))\n",
    "reg_m0.add(Dense(2, activation=\"relu\", kernel_initializer='zero', bias_initializer='zeros'))\n",
    "reg_m0.add(Dense(1, activation=\"sigmoid\", kernel_initializer='zero', bias_initializer='zeros'))\n",
    "\n",
    "reg_m0.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=alpha))\n",
    "\n",
    "print(\"Training : RN theta=0 ...\")\n",
    "results_m0 = reg_m0.fit(X_trains, Y_train, validation_data=(X_tests, Y_test), epochs=NBR_IT, verbose=0)\n",
    "\n",
    "reg_m1 = Sequential()\n",
    "reg_m1.add(Dense(4, activation=\"relu\", kernel_initializer='one', bias_initializer='one'))\n",
    "reg_m1.add(Dense(2, activation=\"relu\", kernel_initializer='one', bias_initializer='one'))\n",
    "reg_m1.add(Dense(1, activation=\"sigmoid\", kernel_initializer='one', bias_initializer='one'))\n",
    "\n",
    "reg_m1.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=alpha))\n",
    "\n",
    "print(\"Training : RN theta=1 ...\")\n",
    "results_m1 = reg_m1.fit(X_trains, Y_train, validation_data=(X_tests, Y_test), epochs=NBR_IT, verbose=0)\n",
    "\n",
    "\n",
    "reg_mr = Sequential()\n",
    "reg_mr.add(Dense(4, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "reg_mr.add(Dense(2, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "reg_mr.add(Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "\n",
    "reg_mr.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=alpha))\n",
    "\n",
    "print(\"Training : RN theta=random ...\")\n",
    "results_mr = reg_mr.fit(X_trains, Y_train, validation_data=(X_tests, Y_test), epochs=NBR_IT, verbose=0)\n",
    "\n",
    "# the first 3 iterations are not displayed, while the model stabilizes\n",
    "# otherwise, a model can have a large value compared to the others \n",
    "# therefore, we can't visualize the convergence of the others\n",
    "\n",
    "IT_range = range(NBR_IT)[3:]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,5))\n",
    "\n",
    "ax1.title.set_text(\"Entrainement\")\n",
    "ax1.plot(IT_range, results.history[\"loss\"][3:], label=\"Regression theta=0\")\n",
    "ax1.plot(IT_range, resultsr.history[\"loss\"][3:], label=\"Regression theta=random\")\n",
    "ax1.plot(IT_range, results_m0.history[\"loss\"][3:], label=\"RN thera=0\")\n",
    "ax1.plot(IT_range, results_m1.history[\"loss\"][3:], label=\"RN theta=1\")\n",
    "ax1.plot(IT_range, results_mr.history[\"loss\"][3:], label=\"RN theta=random\")\n",
    "ax1.set(xlabel='iteration', ylabel='erreur')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.title.set_text(\"Validation\")\n",
    "ax2.plot(IT_range, results.history[\"val_loss\"][3:], label=\"Regression theta=0\")\n",
    "ax2.plot(IT_range, resultsr.history[\"val_loss\"][3:], label=\"Regression theta=random\")\n",
    "ax2.plot(IT_range, results_m0.history[\"val_loss\"][3:], label=\"RN thera=0\")\n",
    "ax2.plot(IT_range, results_m1.history[\"val_loss\"][3:], label=\"RN thera=1\")\n",
    "ax2.plot(IT_range, results_mr.history[\"val_loss\"][3:], label=\"RN thera=random\")\n",
    "ax2.set(xlabel='iteration', ylabel='erreur')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de64ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do it (manage the loop ourselves)\n",
    "# This is just for those interested in improving training time\n",
    "# Training models in a single loop \n",
    "\n",
    "alpha=0.1\n",
    "NBR_IT = 200\n",
    "\n",
    "M, N = X_train.shape\n",
    "\n",
    "inputs = Input(shape=(None,N), name=\"digits\")\n",
    "\n",
    "x1 = Dense(4, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')(inputs)\n",
    "x2 = Dense(2, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')(x1)\n",
    "outputs = Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform', name=\"predictions\")(x2)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "loss=tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer=tf.keras.optimizers.SGD(learning_rate=alpha)\n",
    "\n",
    "print(\"Training ...\")\n",
    "results = reg0.fit(X_trains, Y_train, epochs=NBR_IT, verbose=0)\n",
    "\n",
    "hist = []\n",
    "for epoch in range(NBR_IT):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Logits for this minibatch\n",
    "        H_train = model(X_trains, training=True)\n",
    "        # Compute the loss value for this minibatch.\n",
    "        J_train = loss(Y_train, H_train)\n",
    "    dJ_train = tape.gradient(J_train, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(dJ_train, model.trainable_weights))\n",
    "    hist.append(J_train.numpy().mean())\n",
    "        \n",
    "plt.plot(range(NBR_IT), hist, label=\"neurons random\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d061cb",
   "metadata": {},
   "source": [
    "**\\[Q1\\]: Analyze the results**\n",
    "\n",
    "1. We can easily notice that the classical models (from the last labs) are faster than the neural network models (in terms of iterations and in terms of time). Why is this ? what do you think is the root cause of this ?\n",
    "\n",
    "2. We observe that **RN0** does not improve (it stagnates from the first iterations). Explain why.\n",
    "3. We observe that **RN1** is improving compared to **RN0**, but it is rapidly becoming more stagnant compared to **RNR**. Explain why.\n",
    "4. Noticing the validation, what is the relationship between the number of layers, the complexity of the problem, the number/quality of the data and the learning problems (under/over). Mention all combinations that can cause problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbad0f",
   "metadata": {},
   "source": [
    "[A1:Begin]\n",
    "\n",
    "**Answer 01**\n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "4. ...\n",
    "\n",
    "\n",
    "[A1:End]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b29e6",
   "metadata": {},
   "source": [
    "#### II.2. Activation functions\n",
    "\n",
    "Here, we want to test the usefulness of the activation functions in the hidden layers and the output layer.\n",
    "To do this, we trained 5 models and retrieve the training error in each iteration. \n",
    "The tested models are :\n",
    "- **relu->sigmoid** : a Neural Network with **relu** in the hidden layers and **sigmoid** in the output layer.\n",
    "- **sigmoid->sigmoid** : a Neural Network with **sigmoid** in the hidden layers and **sigmoid** in the output layer.\n",
    "- **tanh->sigmoid** : a Neural Network with **tanh** in the hidden layers and **sigmoid** in the output layer.\n",
    "- **sigmoid->relu** : a Neural Network with **sigmoid** in the hidden layers and **relu** n the output layer.\n",
    "- **relu->relu** : a Neural Network with **relu** in the hidden layers and **relu** in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512bbcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.01\n",
    "NBR_IT = 100\n",
    "\n",
    "M, N = X_train.shape\n",
    "\n",
    "L1 = 2\n",
    "L2 = 2\n",
    "\n",
    "m1 = Sequential()\n",
    "m1.add(Dense(L1, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m1.add(Dense(L2, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m1.add(Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m1.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=alpha))\n",
    "print(\"Training elu->sigmoid ...\")\n",
    "results_m1 = m1.fit(X_trains, Y_train, epochs=NBR_IT, verbose=0)\n",
    "\n",
    "m2 = Sequential()\n",
    "m2.add(Dense(L1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m2.add(Dense(L2, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m2.add(Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m2.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=alpha))\n",
    "print(\"Training sigmoid->sigmoid...\")\n",
    "results_m2 = m2.fit(X_trains, Y_train, epochs=NBR_IT, verbose=0)\n",
    "\n",
    "m3 = Sequential()\n",
    "m3.add(Dense(L1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m3.add(Dense(L2, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m3.add(Dense(1, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m3.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=alpha))\n",
    "print(\"Training tanh->sigmoid ...\")\n",
    "results_m3 = m3.fit(X_trains, Y_train, epochs=NBR_IT, verbose=0)\n",
    "\n",
    "m4 = Sequential()\n",
    "m4.add(Dense(L1, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m4.add(Dense(L2, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m4.add(Dense(1, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m4.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=alpha))\n",
    "print(\"Training sigmoid->relu ...\")\n",
    "results_m4 = m4.fit(X_trains, Y_train, epochs=NBR_IT, verbose=0)\n",
    "\n",
    "m5 = Sequential()\n",
    "m5.add(Dense(L1, activation=\"tanh\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m5.add(Dense(L2, activation=\"tanh\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m5.add(Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "m5.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=alpha))\n",
    "print(\"Training relu->relu ...\")\n",
    "results_m5 = m5.fit(X_trains, Y_train, epochs=NBR_IT, verbose=0)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,5))\n",
    "\n",
    "ax1.plot(range(NBR_IT), results_m1.history[\"loss\"], label=\"relu->sigmoid\")\n",
    "ax1.plot(range(NBR_IT), results_m2.history[\"loss\"], label=\"sigmoid->sigmoid\")\n",
    "ax1.plot(range(NBR_IT), results_m5.history[\"loss\"], label=\"tanh->sigmoid\")\n",
    "ax1.legend()\n",
    "ax2.plot(range(NBR_IT), results_m3.history[\"loss\"], label=\"sigmoid->relu\")\n",
    "ax2.plot(range(NBR_IT), results_m4.history[\"loss\"], label=\"relu->relu\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ae119",
   "metadata": {},
   "source": [
    "**\\[Q2\\]: Analyze the results :**\n",
    "\n",
    "1. We notice that the **sigmoid->sigmoid** model has stagnated rapidly. Explain how?\n",
    "2. We notice that the model converges more rapidly (in terms of number of iterations) compared to the two models with **sigmoid** output. Why does this happen?\n",
    "3. We notice that the models with **relu** are not stable; at each execution, we will have a different diagram (sometimes improvement, sometimes deterioration, etc.). It should be noted that the random initialization is not the source of the problem since there are other similar but stable models. So, why did we have this behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd2d016",
   "metadata": {},
   "source": [
    "[A2:Begin]\n",
    "\n",
    "**Answer 02**\n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "\n",
    "[A2:End]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199edd47",
   "metadata": {},
   "source": [
    "#### II.3. Optimization functions\n",
    "\n",
    "Here, we want to test the different optimization functions.\n",
    "To do this, we trained 4 models and retrieved the training error in each iteration. \n",
    "The tested models are :\n",
    "\n",
    "- **GD** : a neural network trained with gradient descent optimisation algorithm\n",
    "- **Adagrad** : a neural network trained with AdaGrad optimisation algorithm\n",
    "- **RMSprop** : a neural network trained with RMSprop optimisation algorithm\n",
    "- **Adam** : a neural network trained with Adam optimisation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.01\n",
    "NBR_IT = 300\n",
    "\n",
    "M, N = X_train.shape\n",
    "\n",
    "L1 = 2\n",
    "L2 = 2\n",
    "\n",
    "# Modèle avec Descente du gradient\n",
    "model_sgd = Sequential()\n",
    "model_sgd.add(Dense(L1, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "model_sgd.add(Dense(L2, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "model_sgd.add(Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "model_sgd.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=alpha))\n",
    "print(\"Training with gradient descent optimisation algorithm ...\")\n",
    "results_sgd = model_sgd.fit(X_trains, Y_train, epochs=NBR_IT, verbose=0)\n",
    "\n",
    "# Modèle avec Adaptative gradient \n",
    "model_adagrad = Sequential()\n",
    "model_adagrad.add(Dense(L1, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "model_adagrad.add(Dense(L2, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "model_adagrad.add(Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "model_adagrad.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adagrad(learning_rate=alpha))\n",
    "print(\"Training with AdaGrad optimisation algorithm...\")\n",
    "results_adagrad = model_adagrad.fit(X_trains, Y_train, epochs=NBR_IT, verbose=0)\n",
    "\n",
    "\n",
    "# Modèle avec RMSprop\n",
    "model_rmsprop = Sequential()\n",
    "model_rmsprop.add(Dense(L1, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "model_rmsprop.add(Dense(L2, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "model_rmsprop.add(Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "model_rmsprop.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=alpha))\n",
    "print(\"Training with RMSprop optimisation algorithm ...\")\n",
    "results_rmsprop = model_rmsprop.fit(X_trains, Y_train, epochs=NBR_IT, verbose=0)\n",
    "\n",
    "\n",
    "# Modèle avec Adam\n",
    "model_adam = Sequential()\n",
    "model_adam.add(Dense(L1, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "model_adam.add(Dense(L2, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "model_adam.add(Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "model_adam.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "print(\"Training Adam optimisation algorithm ...\")\n",
    "results_adam = model_adam.fit(X_trains, Y_train, validation_data=(X_tests, Y_test), epochs=NBR_IT, verbose=0)\n",
    "\n",
    "# on n'affiche pas les 3 premières itérations, le temps que le modèle se stabilise\n",
    "# sinon, un modèle peut avoir une grande valeur par rapport aux autres \n",
    "# donc, on ne peut pas visualiser la convergence des autres\n",
    "IT_range = range(NBR_IT)[3:]\n",
    "\n",
    "plt.plot(IT_range, results_sgd.history[\"loss\"][3:], label=\"GD\")\n",
    "plt.plot(IT_range, results_adagrad.history[\"loss\"][3:], label=\"Adagrad\")\n",
    "plt.plot(IT_range, results_rmsprop.history[\"loss\"][3:], label=\"RMSprop\")\n",
    "plt.plot(IT_range, results_adam.history[\"loss\"][3:], label=\"Adam\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8dfcc",
   "metadata": {},
   "source": [
    "**\\[Q3\\]: Analyze the results:**\n",
    "\n",
    "- We notice that the **GD** model converges faster than **AdaGrad**. Why is this?\n",
    "- Why does **RMSprop** converge faster than **AdaGrad**, yet their equations are almost similar? (here, you have to explain the contribution in the equation of the first one compared to the second)\n",
    "- When we run it several times, we notice that Adam is more stable. Why is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49507fae",
   "metadata": {},
   "source": [
    "[A3:Begin]\n",
    "\n",
    "**Answer 03**\n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "\n",
    "[A3:End]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9339a3",
   "metadata": {},
   "source": [
    "## III. Improvement of a model : Classifying MNIST Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed65bb4",
   "metadata": {},
   "source": [
    "- We will use the MNIST dataset. \n",
    "- Here, we use only 100 samples for training (10 for each digit) and 30 for validation (3 for each digit). \n",
    "- So, the model will not be powerful; it is just to build some applications in image processing.\n",
    "- The dataset contains 28*28 pixels images representing manually written digits. Each pixel is a number between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4245da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout, UpSampling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# The following two instructions will download the whole dataset\n",
    "# mnist = tf.keras.datasets.mnist\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "train = pd.read_csv(\"datasets/mnist_train.csv\")\n",
    "test = pd.read_csv(\"datasets/mnist_test.csv\")\n",
    "\n",
    "x_train = train.iloc[:, :-1].values # First columns \n",
    "y_train = train.iloc[:,-1].values   # last column\n",
    "x_test = test.iloc[:, :-1].values   # First columns \n",
    "y_test = test.iloc[:,-1].values     # Last column \n",
    "\n",
    "x_train = np.reshape(x_train, [-1, 28, 28, 1])\n",
    "x_test = np.reshape(x_test, [-1, 28, 28, 1])\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    plt.title(\"N: \" + str(y_train[i]))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3bf993",
   "metadata": {},
   "source": [
    "### Classification of the numbers in the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b88e7",
   "metadata": {},
   "source": [
    "Here we try to train a model that recognizes the numbers from 0 to 9. \n",
    "There are two models: \n",
    "- In the first cell, it is a basic model\n",
    "- In the second cell, it is a model identical to the first one. Except, you have to change it by adding layers: Dropout, MaxPooling2D, etc. \n",
    "\n",
    "NB ; The second model should be an improvement. You have to discuss how you improved it, i.e. if you added another layer, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBR_IT = 100\n",
    "epochs = range(NBR_IT)\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "detect_basic = Sequential()\n",
    "detect_basic.add(Dense(128, activation=\"relu\", input_shape=input_shape))\n",
    "detect_basic.add(Flatten())\n",
    "detect_basic.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "detect_basic.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adadelta(),\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Training ...\")\n",
    "results = detect_basic.fit(x_train, y_train_onehot, epochs=NBR_IT, validation_data=(x_test, y_test_onehot), verbose=0)\n",
    "\n",
    "# uncomment the following for evaluation\n",
    "# print(\"Evaluation ...\")\n",
    "# score = detect_basic.evaluate(x_test, y_test_onehot, verbose=0)\n",
    "\n",
    "history = results.history\n",
    "couts_train = history[\"loss\"]\n",
    "couts_test = history[\"val_loss\"]\n",
    "accuracy_train = history[\"accuracy\"]\n",
    "accuracy_test = history[\"val_accuracy\"]\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n",
    "ax[0].plot(epochs, couts_train, color=\"orange\", label=\"Training cost\")\n",
    "ax[0].plot(epochs, couts_test, color=\"blue\", label=\"Test evaluation Cost\")\n",
    "ax[1].plot(epochs, accuracy_train, color=\"orange\", label=\"Training accuracy\")\n",
    "ax[1].plot(epochs, accuracy_test, color=\"blue\", label=\"Test evaluatio accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af150ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO improve the classification model (CNN)\n",
    "# =============== Modifier ici ===================\n",
    "detect_good = Sequential()\n",
    "detect_good.add(Dense(128, activation=\"relu\", input_shape=input_shape))\n",
    "detect_good.add(Flatten())\n",
    "detect_good.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "detect_good.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adadelta(),\n",
    "    metrics=[\"accuracy\"])\n",
    "# =============== Fin modification ================\n",
    "\n",
    "\n",
    "print(\"Training ...\")\n",
    "results = detect_good.fit(x_train, y_train_onehot, epochs=NBR_IT, validation_data=(x_test, y_test_onehot), verbose=0)\n",
    "# print(\"evaluation ...\")\n",
    "# score = detect_basic.evaluate(x_test, y_test_onehot, verbose=0)\n",
    "\n",
    "history = results.history\n",
    "\n",
    "couts_train2 = history[\"loss\"]\n",
    "couts_test2 = history[\"val_loss\"]\n",
    "accuracy_train2 = history[\"accuracy\"]\n",
    "accuracy_test2 = history[\"val_accuracy\"]\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n",
    "ax[0].plot(epochs, couts_train, color=\"orange\", label=\"Training Cost (basique)\")\n",
    "ax[0].plot(epochs, couts_test, color=\"blue\", label=\"Test Cost (basique)\")\n",
    "ax[0].plot(epochs, couts_train2, color=\"red\", label=\"Training Cost (ameliore)\")\n",
    "ax[0].plot(epochs, couts_test2, color=\"green\", label=\"Test Cost  (ameliore)\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(epochs, accuracy_train, color=\"orange\", label=\"Training accuracy (basique)\")\n",
    "ax[1].plot(epochs, accuracy_test, color=\"blue\", label=\"Test accuracy (basique)\")\n",
    "ax[1].plot(epochs, accuracy_train2, color=\"red\", label=\"Training accuracy (ameliore)\")\n",
    "ax[1].plot(epochs, accuracy_test2, color=\"green\", label=\"Test accuracy (ameliore)\")\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e79d413",
   "metadata": {},
   "source": [
    "**Question 4**:\n",
    "\n",
    "The second model should be an improvement. You have to discuss how you improved it, i.e. if you added another layer, why? what layer you add ? how did you choose wich layer to stack in your Neural Net archotecture ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3af0ca",
   "metadata": {},
   "source": [
    "[A4:Begin]\n",
    "\n",
    "**Discussion 04:**\n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "\n",
    "[A4:End]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
